{
  
    
        "post0": {
            "title": "Machine learning error correction codes",
            "content": "try: import optax except ImportError: !pip install optax try: from mynimize import * except ImportError: !git clone https://github.com/idnm/mynimize !git reset --hard e11daa3396ef7682fccf744ce3dce0262cbbfac2 from mynimize.main import * from collections import namedtuple from functools import reduce from jax.scipy.linalg import expm from jax import random from scipy.stats import unitary_group . . Introduction . Quantum error correction codes (QECC) are crucial to the prospects of the quantum technology as well as extremely interesting from a theoretical point of view. Despite similarities to the classical error correction, there are fundamental distinctions, too. A naive attempt to generalize classical codes to work with quantum bits faces several conceptual challenges, such as no-cloning theorem and decoherence induced by a measurement. I got curious how far one can go in constructing a simple quantum ECC without worrying too much about those, using a simple machine learning model with little to no physics assumptions underneath. I think the experiment worked out pretty well, for instance, I&#39;ll show how to get a 5 qubit code. Sure, most things appear to be obvious in retrospect, and rediscovering something that you know is possible is a completely different thing. Still, I found this to be a very interesting exercise with a couple of bonus take-aways, such as: . Measurements are not a necessary part of the error correction. Decoding can be a unitary operation (at least for some codes). | You can train the model on unitary errors only, general single-qubit error channels will be accounted for for free. | . Classical repetition code . Following 99% of error correction tutorials I will start with the classical repetition code. And you know what, I do not even feel guilty, this is a great time-tested warm up. So, we are sending a classical bit along a noisy channel and it is flipped with a probability $p$. The error probability can be suppressed if we are willing to send more bits. Namely, instead of sending 0 we send 000, instead of 1 we send 111. This is called encoding, we encoded a single logical bit into several physical bits. Due to errors, the message 000 can be corrupted in several ways: . 000 : no corruption, with probability $(1-p)^3$ | 100, 010, 001: single corrupted bit, with probability $3 p(1-p)^2$ | 110, 011, 101: two corrupted bits, with probability $3 p^2(1-p)$ | 111: all bits flipped, with probability $p^3$ | Same holds for the 111 message. . Now, if the receiver sees any message except 000 or 111 he knows there was an error somewhere. He can try to fix the error by taking a majority vote, e.g. he assumes that 100 means 0 while 101 means 1. If the single-bit errors are much more likely than two-bit or three-bit errors, this decoding strategy works. More precisely, it succeeds in cases (1) and (2) but fails in cases (3) and (4). The overall success probability is therefore $(1-p)^3+3p(1-p)^2$ and for $p&gt;1/2$ it is in fact greater than $1-p$, the success probability of the unencoded message. . Conceptual difficulties with QECC . Now instead of sending a classical bit we want to send a qubit, also subject to noise. Can we use a similar strategy to protect the quantum bit? Textbooks often mention several apparent problems that make the quantum case sufficiently different from the classical. . Qubit states are continuous. Instead of sending just 0 or 1 we need to be able to send an arbitrary superposition $| psi rangle= alpha|0 rangle+ beta|1 rangle$. | As a consequence, errors are also continuous. For example, instead of a full bit flip $X$ we can have &#39;just a bit&#39; of a bit flip $R_X( theta)= cos( theta/2)-i X sin( theta/2)$ with very small $ theta$. There are also additional error types with no classical counterparts, such as the phase flip error $Z$. General single-qubit unitary error is a linear combination $U= alpha_0+ alpha_1 X+ alpha_2 Y+ alpha_3 Z$. | Quantum states can not be cloned. This means that for an unknown quantum state $| psi rangle$ we can not construct and transmit e.g. $| psi rangle otimes| psi rangle otimes| psi rangle$ as a plain generalization of the repetition code, although that would definitely help. | When the message is received, we need to look at it to decide if there was an error and choose a correction. But looking at the quantum states can break the coherence that we were looking to preserve. | We now know that all these issues can be elegantly resolved and the modern theory of error correction is rich and beautiful. My personal agenda for this small project was to see how far can one go with a black-box approach, sweeping all conceptual problems under the rug. The main two assumptions are . We need to use several physical qubits to safeguard a single logical qubit. | We only try to protect against single-qubit errors. | . . Machine learning QECC . Here is an architecture that I have in mind. . . First, we embed a logical qubit into $n$ physical ones. I will do this in the simplest possible way $| psi rangle to | psi rangle otimes |0 rangle^{n-1}$, i.e. assuming that the first physical qubit is the logical state to be transmitted while other physical qubits are initialized in $|0 rangle$ states. The initial embedding is in fact irrelevant, because after that I allow for an arbitrary encoding transformation $U_{encoding}$. It is only required to be unitary. The encoding stage does the heavy lifting, and the encoding unitary is the main variable to be optimized. After that we add an error layer, which can consist of arbitrary single-qubit unitary errors. Then goes the decoding layer, which we will trust to recover the information about the logical qubit (it will also be trained). I will assume that the decoding layer is also a unitary. At the final step, the physical qubit state must somehow be projected onto the single-qubit state which will be our final, received and corrected state. . If you are familiar with quantum error correction, the assumption that decoding is a unitary operation and hence makes no explicit reference to syndrome measurements and things of that sort may look suspicious. We&#39;ll see that it works, and make some comments afterwards. . Design choices . Projecting the final state . There are still details to be filled in. One is to specify how to get a single-qubit logical state from the final state of the physical qubits. Similarly to the embedding step, I will assume that the relevant information is contained exclusively in the first physical qubit. Then, successful error correction implies that the first physical qubit is unentangled with the others after the decoding step and has the same state it had before the encoding. . $$| psi rangle otimes |0 rangle^{n-1} to text{Encoding+Error+Decoding} to | psi rangle otimes |e rangle_{n-1}$$ . Note that the rest of the physical qubits will end up in different states $|e rangle_{n-1}$ depending on the error that have been corrected. Requiring that the final state is $| psi rangle otimes |0 rangle^{n-1}$ regardless of the error is too strong and can not be satisfied for any interesting set of errors. . Dealing with continuum . Next, how do we deal with the continuum of states and errors? I guess that a truly black-box approach would be to generate a large set of initial states and single-qubit errors and train the model using all this data. If successful, check on the test data to exclude overfitting. I&#39;m sure that would work, but here I will take a shortcut and exploit the linearity of the whole construction. Denote by $U(E)$ the full unitary of the encoding+error+correction process, for some error $E$ . $$U(E)=U_{decoding} , , U_{error}(E) , , U_{encoding} .$$ . For a given initial state $| psi rangle= alpha |0 rangle+ beta |1 rangle$ and a fixed error $E$, the final state can be reconstructed from the action on $|0 rangle$ and $|1 rangle$ states . $$| psi rangle otimes |0 rangle^{n-1}= alpha , , U(E) |0 rangle otimes|0 rangle^{n-1}+ beta , , U(E)|1 rangle otimes|0 rangle^{n-1} .$$ . Similarly, if we can correct errors corresponding to $X, Y$ and $Z$ unitaries on a given qubit, we will be able to correct an arbitrary linear combination of them, which is unitary. Indeed, say we can correct both $X$ and $Y$ errors begin{align*} | psi rangle otimes |0 rangle^{n-1} to U(X) to | psi rangle otimes |x rangle_{n-1} , | psi rangle otimes |0 rangle^{n-1} to U(Y) to | psi rangle otimes |y rangle_{n-1} . end{align*} Then their unitary linear combination will also be corrected in a sence that the state of the first physical qubit is the original encoded state begin{align*} | psi rangle otimes |0 rangle^{n-1} to U(aX+bY) to | psi rangle otimes left(a|x rangle_{n-1}+b|y rangle_{n-1} right) . end{align*} In fact, correcting $X,Y$ and $Z$ errors on any of the qubits is sufficient to correct their arbitrary linear combination, including non-unitary ones and those acting on different qubits. More on that later. . Implementation . Here is a code that implements the model. I do no use any quantum framework and deal with unitary matrices directly. That requires making a few tensor products here and there, but nothing cumbersome. I include the code right below in order to make this notebook/post self-contained, but do not go into detailed explanations. Here are several technical highlights though. . I use JAX as a numerical optimization backend. This why all the jnps instead of nps. | I take a very low-key approach to optimization over unitary matrices, parametrizing them by Hermitian matrices $U=e^{i H}$. Basis in Hermitian matrices can be chosen to consist of $e_{ii}, e_{ij}+e_{ji}$ and $i(e_{ij}-e_{ji})$, where $e_{ij}$ is a matrix with all elements zero except one at position $ij$. Matrix exponentiation is an expensive procedure and to scale the code to more qubits a better parametrization of the unitary group is required, e.g. as done here QGOPT. | I choose the loss associated with an error correction process $U(E)$ in the following way. Let $| Psi_0 rangle$ be the image of $|0 rangle$ and $| Psi_1 rangle$ of $|1 rangle$ $$| Psi_0 rangle=U(E)|0 rangle otimes |0 rangle^{n-1}, quad | Psi_1 rangle=U(E)|1 rangle otimes |0 rangle^{n-1} . $$ The loss is $$L(E) = 2-L_Z-L_X, qquad L_Z= langle Psi_0|Z_1| Psi_0 rangle, qquad L_X= operatorname{Re} langle Psi_0|X_1| Psi_1 rangle .$$ The term $L_Z$ is maximal $L_Z=1$ when $| Psi_0 rangle=|0 rangle otimes |e rangle_{n}$, i.e. when the $|0 rangle$ state of the first physical qubit is preserved by the error correction. If $|0 rangle$ is mapped to a mixed state or to a pure state different from $|0 rangle$ we have $L_Z&lt;1$. The term $L_X$ is maximal when the $| Psi_1 rangle=X_1 | Psi_0 rangle$. If $|0 rangle otimes|0 rangle^{n-1} to|0 rangle otimes |e rangle_{n}$, this condition implies that $|1 rangle otimes|0 rangle^{n-1} to |1 rangle otimes |e rangle_{n}$. By linearity, this is sufficient for an arbitray input state to be corrected when subject to this error. | The total loss is the sum of individual losses over all $X, Y, Z$ errors acting on each qubit $$L(E)= sum_{i=1}^{n} left(L(X_i)+L(Y_i)+L(Z_i) right) .$$ However, we may wish to correct only a subset of errors, say only $X$ errors. Then include only those in the loss function. | . # Pauli matrices. x_mat = jnp.array([[0, 1], [1, 0]]) y_mat = jnp.array([[0, -1j], [1j, 0]], dtype=jnp.complex64) z_mat = jnp.array([[1, 0], [0, -1]]) pauli = (jnp.identity(2), x_mat, y_mat, z_mat) # Parametrized unitary matrices. class UnitaryLayer: def __init__(self, num_qubits): self.num_qubits = num_qubits self.num_params = 4**num_qubits @staticmethod def hermitian_basis(num_qubits): d = 2**num_qubits diag_basis = [jnp.zeros((d, d), dtype=jnp.complex64).at[i, i].set(1) for i in range(d)] off_diag_real_basis = [jnp.zeros((d, d), dtype=jnp.complex64).at[i, j].set(1).at[j, i].set(1) for i in range(d) for j in range(i)] off_diag_im_basis = [jnp.zeros((d, d), dtype=jnp.complex64).at[i, j].set(1j).at[j, i].set(-1j) for i in range(d) for j in range(i)] return jnp.array(diag_basis+off_diag_real_basis+off_diag_im_basis) def unitary(self, params): generator = jnp.tensordot(self.hermitian_basis(self.num_qubits), params, axes=((0, ), (0, ))) return expm(1j*generator) # Matrices corresponding individual single-qubit errors. class ErrorLayer: def __init__(self, num_qubits, errors=pauli): self.num_qubits = num_qubits self.errors = errors def unitary(self, q, e): error_list = [self.errors[e] if i==q else jnp.identity(2) for i in range(self.num_qubits)] return reduce(jnp.kron, error_list) def all_error_unitaries(self): single_error_unitaries = [self.unitary(q, e) for q in range(self.num_qubits) for e in range(len(self.errors))] id_error = jnp.identity(2**self.num_qubits) return jnp.array([id_error]+single_error_unitaries) # Model parameters as a namedtuple. ecc_params = namedtuple(&#39;ECCparams&#39;, [&#39;encoding_params&#39;, &#39;decoding_params&#39;]) class ECCmodel: def __init__(self, num_qubits, error_layer, params=None): self.num_qubits = num_qubits self.error_layer = error_layer self.encoding_layer = UnitaryLayer(num_qubits) self.decoding_layer = UnitaryLayer(num_qubits) self.params = params @staticmethod def embed(initial_state, num_qubits): &quot;&quot;&quot;Take |psi&gt; and output |psi&gt;|0,0,0, ...&gt; &quot;&quot;&quot; return jnp.kron(initial_state, jnp.zeros(2**(num_qubits-1)).at[0].set(1)) def final_state(self, initial_state, encoding_unitary, decoding_unitary, error_unitary): s = decoding_unitary @ error_unitary @ encoding_unitary @ initial_state return s def loss(self, params, error_unitary): encoding_unitary = self.encoding_layer.unitary(params.encoding_params) decoding_unitary = self.decoding_layer.unitary(params.decoding_params) final_states = [] for initial_state_1q in [[1,0], [0,1]]: initial_state = self.embed(jnp.array(initial_state_1q, dtype=jnp.complex64), self.num_qubits) final_state = self.final_state(initial_state, encoding_unitary, decoding_unitary, error_unitary) final_states.append(final_state) Psi_0, Psi_1 = final_states X1 = reduce(jnp.kron, [x_mat]+[jnp.identity(2)]*(self.num_qubits-1)) Z1 = reduce(jnp.kron, [z_mat]+[jnp.identity(2)]*(self.num_qubits-1)) Z_avg = jnp.real(Psi_0.conj() @ Z1 @ Psi_0).sum() X_off_diag = (Psi_0.conj() @ X1 @ Psi_1).sum() return 2-Z_avg-jnp.real(X_off_diag) def train(self, opt_options=OptOptions(num_iterations=1000)): error_unitaries = self.error_layer.all_error_unitaries() def loss(params): losses = vmap(lambda error_u: self.loss(params, error_u))(error_unitaries) return losses.sum()/len(error_unitaries) initial_params = random.uniform(random.PRNGKey(opt_options.random_seed), shape=(2, self.encoding_layer.num_params)) initial_params = [ecc_params(initial_params[0], initial_params[1])] # `mynimize` is just my custom optimization routine with a JAX backend. results = mynimize(loss, initial_params, opt_options) self.params = results.best_result.best_params return results.best_result . WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) . Quantum repetition code . Now let us put the model to use. It is well known that with three physical qubits one can protect a logical qubit from $X$ erorrs. This is a generalization of the classical repetition code. Let&#39;s see if our model can do that. . %%time num_qubits = 3 error_layer = ErrorLayer(num_qubits, errors=[x_mat]) model = ECCmodel(num_qubits, error_layer) result = model.train(OptOptions(num_iterations=1000)) result.plot_loss_history(); plt.xlabel(&#39;Iteration&#39;); plt.title(&#39;Loss history&#39;); . CPU times: user 15.6 s, sys: 62.7 ms, total: 15.7 s Wall time: 15.6 s . Text(0.5, 1.0, &#39;Loss history&#39;) . OK, the optimization is clearly successfull. Of course there is always a chance that there are mistakes in the code or in the definition of the loss function itself. I will give a more thorough verification for the 5-qubit code later. By modifying and re-running the cell above you can also do some simple sanity checks -- see if the same results can be achieved with fewer qubits (num_qubits$ to$2) or if more errors can be corrected (errors$ to$[x_mat, y_mat]). Neither works, of course. . Standard description of the quantum repetition code . Now that we have seen that a black-box approach works it is instructive to revisit the usual construction of the error correcting codes. Here is how the quantum repetition code, which is able to correct $X$ errors, works. Encoding is done as follows . $$| psi rangle= alpha|0 rangle+ beta|1 rangle to alpha |000 rangle+ beta|111 rangle .$$ . This does not violate the no-cloning theorem because the new state is not $| psi rangle otimes| psi rangle otimes| psi rangle$. Coefficients $ alpha$ and $ beta$, which define the state, are not copied. This encoding can be done with the following circuit . . Now say there was an $X$ error acting on the first qubit during the transmission $$ alpha |000 rangle+ beta|111 rangle to X_1 to alpha |100 rangle+ beta|011 rangle$$ Can we detect an correct it? The problem is that measuring any of the qubits individually destroys their coherent superposition. A workaround is to make collective measurements $Z_1Z_2$ and $Z_1Z_3$, known as parity checks. Both terms in the corrupted decomposition have the same eigenvalues and hence coherence is preserved. Parity checks allow to identify qubit 1 as corrupted, and correct the error by applying $X_1$. . Can we do without a measurment? . Measurements and post-selected correction operators were not part of our model, where the decoder is unitary. Is there something wrong with our approach, or measurements are not strictly necessary? I do think they aren&#39;t, but you would not be able to tell from most of the introductory literature. To illustrate the situation for the repetition code, I came up with the following unitary circuit, which can correct/decode any single $X$ error in the repetition code . . It is straightforward to check that it transforms vectors with single $X$ errors as follows. begin{align*} I_{}: quad |000 rangle to |000 rangle, quad |111 rangle to |100 rangle X_1: quad|100 rangle to |011 rangle, quad |011 rangle to |111 rangle X_2: quad|010 rangle to |101 rangle, quad |010 rangle to |110 rangle X_3: quad|001 rangle to |001 rangle, quad |110 rangle to |101 rangle end{align*} Important things to note here are that the first qubit value becomes the majority vote, while two other qubit registers agree within the same line. I do not claim that our numerical optimization above discovered exactly this circuit and/or the repetition encoding, but it must be something equivalent. . 5 qubit code . The smallest amount of physical qubits that can correct against arbitrary single-qubit errors is known to be 5. Let me sketch a proof. Please! No, it&#39;s not needed to make my points, I just like it a lot. OK? Great! . If a code can correct an arbitrary single-qubit error, it can also recover from the loss of two qubits. If we&#39;d have an ECC with just four qubits, we could separate them into two groups 4=2+2. Each group could recover the encoded state, which produces two copies of it. This violates the no-cloning theorem! Lowering the number of qubits does not help, of course. . Good, let us try to train the model with 5 physical qubits and the error operators that span all single-qubit errors. . %%time num_qubits = 5 error_layer = ErrorLayer(num_qubits, errors=[x_mat, y_mat, z_mat]) model = ECCmodel(num_qubits, error_layer) result = model.train(OptOptions(num_iterations=1500)) result.plot_loss_history(); plt.xlabel(&#39;Iteration&#39;); plt.title(&#39;Loss history&#39;); . CPU times: user 2min 55s, sys: 1.3 s, total: 2min 56s Wall time: 2min 55s . Text(0.5, 1.0, &#39;Loss history&#39;) . The loss function indicates that our model learns a 5-qubit ECC. You may wish to check that it does not work with fewer qubits, e.g. num_qubits$ to$4. Another empirical observation is that if the model is trained on $X$ and $Z$ errors only, it will be able to correct $Y$ errors as well (passes verification below). This does not seem to be guarantied in general, as counter-examples exist. . Verification . To convince you and myself that the loss plot above does reflect learning a genuine ECC here I will carry out an independent check. First let me note that the way errors enter in the model we trained is not completely general. The most general evolution of the initial state under an interaction with an environment is described by a quantum channel . $$| psi rangle to rho = operatorname{Tr}_{n-1} sum {M_a}| Psi rangle langle Psi|M_a^ dagger, qquad | Psi rangle=| psi rangle otimes |0 rangle^{n-1} .$$ . Here $ rho$ is the final density matrix of the first physical qubit, which by our assumption corresponds to the logical qubit after error correction process. The partial trace is taken with respect to the other physical qubits. Matrices $M_a$ are called Kraus operators and could be thought of as a combination $$M_a = U_{decoding} E_a U_{encoding} .$$ We trained our model on cases where the error part $E_a$ in each Kraus operator $M_a$ is a single Pauli operator acting on some qubit, e.g. $E_a=X_2$. A general single-qubit error corresponds to each $E_a$ being a linear combination of single-qubit unitaries $$E_a= sum_{i=1}^{n} c_{ai} U_i.$$ For example, one of them could be something like $E_1 = c_{11}(0.13 X_1+2.7 Y_1)+c_{12} Z_2 + c_{13} (Y_1-0.55 Z_1)$. Kraus operators are not required to be unitary, but only to satisfy the completeness relation $ sum M_a^ dagger M_a=1$. Our model was trained so that $M_a | Psi rangle=| psi rangle otimes | text{some state} rangle$ when $M_a$ only contains Pauli errors acting on a single qubit. However, this equation extends to arbitrary single-qubit errors by linearity. . To perform an independent check I generate a bunch of initial states and generic single-qubit errors $E_a$. I will restrict to channels with single (non-normalized) Kraus operators for simplicity. If (normalized) density matrices $$ rho_a= frac{M_a| Psi rangle langle Psi|M_a^ dagger}{ langle Psi|M_a^ dagger M_a| Psi rangle} $$ reproduce the correlators of the original state for any $M_a$, they surely do for any sum over $M_a$. Thus, I will check that . $$ langle psi|X| psi rangle= operatorname{tr} rho_a X, quad langle psi|Y| psi rangle= operatorname{tr} rho_a Y, quad langle psi|Z| psi rangle= operatorname{tr} rho_a Z .$$ . Essentially, we&#39;ll do the full state tomography of the first physical qubit. This of course should be equivalent to the loss function we used during training, but I think reformulation is useful as an additional consistency check. . key, *keys = random.split(random.PRNGKey(0), 3) # Sample sizes. num_qubits = model.num_qubits num_initial_states = 100 num_errors = 100 # Initial states, drawn at random and normalized. initial_states = random.uniform(keys[0], shape=(2, num_initial_states, 2)) # 2x real components initial_states = initial_states[0]+1j*initial_states[1] # combine into 1x complext components def norm_state(s): return jnp.sqrt(jnp.real(s.conj()*s).sum()) norms = [norm_state(s) for s in initial_states] initial_states = [s/norm_state(s) for s in initial_states] ### Defining random linear combinations of single-qubits errors is a bit cumbersome, but purely technical. # Random single-qubit errors. random_1q_unitaries = unitary_group.rvs(2, size=num_errors*num_qubits).reshape(num_errors, num_qubits, 2, 2) random_coefficients = random.uniform(keys[1], (num_errors, num_qubits)) def error_at_position(error_u, i, num_qubits): &quot;&quot;&quot;Takes U and returns tensor product 1 x 1 x ... U x 1 ... x 1 with U at position i.&quot;&quot;&quot; ops = [jnp.identity(2)]*num_qubits ops[i] = error_u return reduce(jnp.kron, ops) def make_error_operator(errors, coeffs): &quot;&quot;&quot;Takes a list of 1q errors and puts error 1 on qubit 1, error 2 on qubit 2, etc, then takes their liner combination.&quot;&quot;&quot; num_qubits = len(errors) full_errors = [error_at_position(u, i, num_qubits) for i, u in enumerate(errors)] return sum([u*c for u, c in zip(full_errors, coeffs)]) errors = [make_error_operator(errors, coeffs) for errors, coeffs in zip(random_1q_unitaries, random_coefficients)] errors = jnp.array(errors) . So finally we have a bunch of initial states and general (non-unitary) single-qubit error operators. Let&#39;s see directly the the model is able to correct them. . params = model.params u_encoding = model.encoding_layer.unitary(params.encoding_params) u_decoding = model.decoding_layer.unitary(params.decoding_params) def density_matrix(s): &quot;&quot;&quot;Density matrix of the first qubit.&quot;&quot;&quot; num_qubits = int(jnp.log2(len(s))) s = s.reshape([2]*num_qubits) axes = list(range(1, num_qubits)) rho = jnp.tensordot(s, s.conj(), axes=(axes, axes)) return rho def tomography_loss(initial_state, error_u): final_state = u_decoding @ error_u @ u_encoding @ ECCmodel.embed(initial_state, num_qubits) # For non-unitary errors the state should be normilized. final_state = final_state/jnp.sqrt(jnp.real((final_state.conj()*final_state).sum())) final_rho = density_matrix(final_state) pauli_averages_initial = [(initial_state.conj() @ p @ initial_state).sum() for p in [x_mat, y_mat, z_mat]] pauli_averages_final = [jnp.trace(final_rho @ p) for p in [x_mat, y_mat, z_mat]] loss = sum([(avg_i - avg_f)**2 for avg_i, avg_f in zip(pauli_averages_initial, pauli_averages_final)]) return jnp.real(loss) losses = [tomography_loss(s, u) for s in initial_states for u in errors] avg_loss = sum(losses)/len(losses) print(f&#39;Average loss from tomography: {float(avg_loss)}&#39;) . Average loss from tomography: 2.6336230973811325e-09 . OK, the average tomographic loss is basically within the machine precision, which makes a strong case for the fact our simple model did learn a genuine ECC code on 5 qubits. This wraps up my experiment! .",
            "url": "https://idnm.github.io/blog/blog/qml/qec/2022/06/16/Machine-learning-error-correction-codes.html",
            "relUrl": "/qml/qec/2022/06/16/Machine-learning-error-correction-codes.html",
            "date": " • Jun 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Some analytic facts about variational quantum algorithms",
            "content": "from qiskit import QuantumCircuit from qiskit.circuit import Parameter from math import pi . . Introduction . Variational quantum algorithms (VQA) is a huge field by now with many prospective applications and a poll of advocates for their potential quantum advantage on the NISQ devices, see e.g. here for a recent review. I usually think of variational quantum algorithms as analogues of the neural networks with a parametrized quantum circuit playing the role of a trainable model. Here is an example of a parametrized quantum circuit: . . Despite certain similarities there are also crucial distinctions between the classical and quantum nets, manifesting both in their functional shapes and trainability properties. Here I will mostly talk about some functional properties of parametrized quantum circuits, and briefly touch on the trainability issues at the end. A whole slew of additional peculiarities arise when you want to run quantum circuits on real quantum hardware and have to deal with errors and stochastic nature of measurements. Here I will ignore these issues completely, effectively assuming that we can run a classical simulation of the quantum circuit. . Hi $ cos$, hi $ sin$! . For a generic neural net the loss function is a highly non-linear function of each weight, due to non-linear activation functions that connect the layers. In contrast, dependence of the parametrized quantum circuits on each single parameter separately is extremely simple. . I will denote the unitary matrix of a parametrized quantum circuit by $U( theta)$, with $ theta$ standing for all the parameters collectively. A simple observation, and really the basis for all of the following discussion, is the following equation begin{align} U( theta_i)=U_0 cos frac{ theta_i}{2}+U_1 sin frac{ theta_i}{2} label{u cos sin} . end{align} Here $ theta_i$ stands for any parameter of the circuit, other are assumed to be fixed. Matrix coefficients $U_0$ and $U_1$ are given by $U_0=U(0)$ and $U_1 = U( pi)$. Relation eqref{u cos sin} follows from the simple fact that all gates typically considered in VQA are of the form begin{align*} G( theta) = e^{-i theta Sigma /2} end{align*} with a generator $ Sigma$ that satisfies $ Sigma^2=1$. Hence, by a generalization of Euler&#39;s formula $e^{i phi}= cos phi+i sin phi$ any gate can be alternatively written as begin{align*} G( theta) = cos frac{ theta}{2}-i Sigma sin frac{ theta}{2} . end{align*} As an example one take any single-qubit Pauli rotation ref, say $R_X( theta)=e^{-i theta X/2}= cos frac{ theta}{2}-iX sin frac{ theta}{2}$. Parametric two-qubit gates, e.g. $R_{ZX}$ gate, usually conform to the same rule. Because of the trigonometric function here the parameters in VQA are often referred to as angles, the terminology which I will follow. . A typical loss function . A subset of the VQA are variational quantum eigensolvers (VQE). A typical loss function in VQE is quadratic in $U( theta)$. For instance, one common goal in VQE is to prepare the ground state of some Hamiltonian $H$ using an ansatz $| psi( theta) rangle = U( theta)|0 rangle$. The relevant cost function to be minimized is . begin{align} L( theta)= langle psi( theta)|H| psi( theta) rangle label{loss VQE} . end{align}In the unitary synthesis problem, that I&#39;ve been recently interested in, the goal is to make the circuit $U( theta)$ equivalent to some target unitary $V$. The relevant loss can be defined as . begin{align*} L( theta)=-| operatorname{Tr} V^ dagger U( theta)|^2 end{align*}You got the idea. Note that while the circuit $U( theta)$ has only two terms eqref{u cos sin} as a function of any angle $ theta_i$, the quadratic loss function will have three terms (note also the period doubling) . begin{align*} L( theta_i)=A cos theta_i+B sin theta_i + C . end{align*}Here $A, B, C$ are functions of all the other angles except for $ theta_i$. They are the only unknowns that specify dependence on any particular angle and can be found with just three evaluations of the loss function, e.g. . begin{align*} A+B = L(0), qquad A+C=L( pi/2), qquad A-C = L(- pi/2) Rightarrow A = frac{L( pi/2)+L(- pi/2)}{2}, qquad C= frac{L( pi/2)-L(- pi/2)}{2}, qquad B = L(0)-A . end{align*} Parameter shift rule . Perhaps the best known consequence of this property is the parameter shift rule for derivatives. First, a bit of a background. Let&#39;s assume we need to estimate the derivative of some function $f(x)$ that we only have numerical access to. Then, there is nothing much better one can do than to use the finite difference approximations. For example, using two function evaluations it is possible to compute the first derivative up to the second approximation order . begin{align*} f&#39;(x)= frac{f(x+ epsilon)-f(x- epsilon)}{2 epsilon}+O( epsilon^2) end{align*}In general, adding one more evaluation point allows to improve the accuracy by one order. However, when you have additional knowledge about the function much more efficient strategy may exists. In particular, for VQE loss functions eqref{loss VQE}, which are basically simple sinusoidals, an exact derivative computation is possible with just two function evaluations . begin{align*} L&#39;( theta_i)=-A sin( theta_i)+B cos{ theta_i}= frac{L( theta_i+ pi/2)-L( theta_i- pi/2)}{2} , end{align*}which follows from $ sin(x+ pi/2)= cos x, , , cos(x+ pi/2)=- sin x$. Having access to exact derivatives generally enhances the performance of the gradient-based optimizers. . Sequential optimization . An interesting extension of this idea, that is apparently much less known than the parameter-shift rule itself, was proposed by several group at roughly the same time ([1], [2], [3], thanks to Vijendran for additional refs). Instead of using structural properties of eqref{loss VQE} to just compute derivatives, one can find the exact minimum of $L( theta_i)$ with respect to any angle $ theta_i$ (assuming other angles are fixed). I.e. instead of a partial derivative one can compute &quot;the partial minimum&quot;. . Indeed, since just three three evaluations fix $L( theta_i)$ completely, and the function itself is rather simple, there is no problem finding $ operatorname{argmin}_{ theta_i}L( theta_i)$ exactly. The explicit formula could be more transparent, but it is a simple trigonometry in the end (double check if you a going to use it!) . begin{align*} theta^*= operatorname{argmin}_{ theta} left(A cos theta+B sin theta+C right)= cases{ arctan frac{B}{A}+ pi,&amp;A&gt;0 arctan frac{B}{A},&amp; A&lt;0} end{align*}With this trick one can bypass gradient-based optimization as follows. Starting from $L( theta_1, theta_2, dots)$ first optimize with respect to the first angle $L( theta_1, theta_2, dots) to L( theta_1^*, theta_2, dots)$. Then optimize with respect to the second $L( theta_1^*, theta_2, dots) to L( theta_1^*, theta_2^*, dots)$. Note that after this step the first angle in general no longer is the best choice, because the second angle has changed. Still, one can continue this procedure further until all angles are updated and then start anew. Each step is guaranteed to decrease the value of the loss function. If the loss landscape is nice overall, this sequential gradient-free optimization may in fact even outperform gradient-based methods. Unfortunately, to my understanding the sequential optimization is unlikely to help with the most crucial problems in the VQE loss landscapes: barren plateaus and local minimums. . Average performance of the VQE . Here comes the original contribution of this blog post, I will show how to compute (semi-efficiently) the average loss begin{align} overline{L}= frac{1}{(2 pi)^p} int prod_{i=1}^p d theta_i langle psi( theta)|H| psi( theta) rangle . end{align} where $p$ is the total number of angles in the parametrized circuit. Why would one be interested in such a quantity? Honestly, I do not know, but hey, this is a blog post and not a paper, so I&#39;ll take a recreational attitude. Seriously though, I&#39;ll use this result in the following section, which however is not well justified either:) . To begin with, let&#39;s make explicit dependence of the parametrized circuit on all of its angles begin{align} U( theta)= sum_{I} U_{I} left( cos frac{ theta}{2} right)^{1-I} left( sin frac{ theta}{2} right)^I label{u exp} . end{align} Here $I$ is a multi-index, a binary string of length $p$, and $ left( cos frac{ theta}{2} right)^{I}$ is an abbreviation for $ prod_{i=1}^p left( cos frac{ theta_i}{2} right)^{I_i}$. For $p=1$ this reduces to eqref{u cos sin}. For $p=2$ we have begin{align*} U( theta)=U_{00} cos frac{ theta_1}{2} cos frac{ theta_2}{2}+U_{01} cos frac{ theta_1}{2} sin frac{ theta_2}{2}+U_{10} sin frac{ theta_1}{2} cos frac{ theta_2}{2}+U_{11} sin frac{ theta_1}{2} sin frac{ theta_2}{2} , end{align*} I think you got the idea. There are exactly $2^p$ terms in this sum. Now let us substitute this expression into the loss function eqref{loss VQE} . begin{align} L( theta)= sum_{I,J} left( cos frac{ theta}{2} right)^{1-I} left( sin frac{ theta}{2} right)^I left( cos frac{ theta}{2} right)^{1-J} left( sin frac{ theta}{2} right)^J langle 0|U_I^ dagger H U_J |0 rangle label{loss exp} . end{align}When we average, all terms with $I neq J$ vanish since $ int_0^{2 pi} d theta sin frac{ theta}{2} cos frac{ theta}{2}=0$. At the same time, all terms with $I=J$ give equal angle integrals $ frac{1}{(2 pi)^p} int prod_{i=1}^p d theta_i left( cos frac{ theta_i}{2} right)^{2 I} left( sin frac{ theta_i}{2} right)^{2-2 I}= frac{1}{2^p}$ since $ int d theta cos^2 frac{ theta}{2}= int d theta sin^2 frac{ theta}{2}= pi$. The results is that begin{align} overline{L}= frac1{2^p} sum_{I} langle 0|U_I^ dagger H U_I |0 rangle label{L average} . end{align} This expression looks simple, but it is a sum with $2^p$ terms, so for any reasonable number of parameters its huuuge. A typical number of parameters is exponential in the number of qubits, so this is the double exponential, not good. I&#39;ve spent multiple hours thinking about how to compute this average more efficiently, but for generic function of the type eqref{u exp} with arbitrary matrix coefficients $U_I$ I didn&#39;t find a way to compute the average loss in less than an exponential in $p$ number of function calls. However, taking into account that $U_I$ are not arbitrary for parametrized quantum circuits, a computation linear in $p$ is possible. The reason is that among exponentially many $U_I$ there is only polynomially many &quot;independent ones&quot;, in a sense that I will now make precise. . For concreteness consider the following toy circuit: . qc = QuantumCircuit(2) qc.cz(0,1) qc.rx(Parameter(&#39;$ theta_1 $&#39;), 0) qc.rx(Parameter(&#39;$ theta_2 $&#39;), 1) qc.rz(Parameter(&#39;$ theta_3 $&#39;), 0) qc.rz(Parameter(&#39;$ theta_4 $&#39;), 1) qc.draw(output=&#39;mpl&#39;) . . Here the entangling gate is the Controlled-Z. This circuit has four parameters and $2^4=16$ associated matrix coefficients $U_I$. What are they, exactly? It is in fact rather simple to understand. If the binary index is $0$ the rotation gate is replaced by the identity, if it is $1$ we insert the generator instead. For example $U_{1110}$ is . qc = QuantumCircuit(2) qc.cz(0,1) qc.x(0) qc.x(1) qc.z(0) qc.global_phase=pi qc.draw(output=&#39;mpl&#39;) . . The global phase arises because $U( theta= pi)=-i Sigma$ for $U( theta)=e^{-i theta Sigma/2}$. Next, consider a more realistic circuit like the one below . . All coefficients $U_I$ arise as $2^p$ different versions of this circuit where each rotation gate is replaced either by an identity or by a generator, just as at the figure above. Although these circuits might all look different, in fact there is just a handful of independent ones. This is due to the following commutation rules, which are easy to check: . . These commutation rules allow to move all the pauli matrices past CZ gates and to the beginning of the circuit. For example, the circuit $U_{1101}$ from above can be alternatively be rewritten as . qc = QuantumCircuit(2) qc.x(0) qc.z(1) qc.x(1) qc.cz(0,1) qc.global_phase=pi qc.draw(output=&#39;mpl&#39;) . . Then, any string of Pauli matrices is equal to $I, X, Z$ or $Y simeq XZ$ up to a global phase. So in the end, up to phase factors there are only $4^n$ linearly independent matrices $U_I$ where $n$ is the number of qubits. The counting $4^n$ follows because after all the generators have been placed at the beginning of the circuit there can be only 4 different operators at each qubit thread. Note also that $4^n$ is precisely the dimension of the unitary group on $n$ qubits. We thus see that all of $2^p$ matrix coefficients can be divided into $4^n$ distinct classes and within each class $U_I=e^{i phi}U_{I&#39;}$. This global phase makes no difference for the averages in eqref{L average} which can therefore be rewritten as . begin{align*} overline{L}= frac{1}{4^n} sum_{c} langle 0|U_c^ dagger H U_c|0 rangle , end{align*}where now the sum is over representatives of distinct classes. So the sum is reduced from $2^p$ to $4^n$ terms. OK, so how does the number of parameters and the number of qubits compare? Is this really a reduction? . Yes it is! First, if you want your parametrized quantum circuit to cover any unitary transformation on $n$ qubits you need at least $p=4^n$ parameters, because this is the dimension of the unitary group. So in this case we have and exponential reduction from $2^{4^n}$ to $4^n$. But even if you only put two rotation gates on each qubit you already got yourself $4^n$ parameters. Adding anything beyond that, as you definitely wish to do, makes the reduction from $2^p$ to $4^n$ essential. Note though that it is still exponential in the number of qubits and would be unfeasible to compute exactly for a large system. . Loss landscape as charge density . OK, here is a brief justification for why I was interested in the average loss in the first place. Generic hamiltonian-agnostic VQE algorithms have in fact lots of trainability issues. One is the presence of the barren plateaus in certain regimes, which means that large portions of the parameter space have vanishing gradients and are bad places for optimizer to be in. Another issue is the presence of local minimums which can be just as bad. So I was wondering if it is possible to somehow use the analytic properties of the VQE loss functions to help mitigate these problems. Here is an idea that probably does not work, but I think still is sort of fun. . Here is an example of a bad loss landscape, sketched in black: . It has many local minimums and flat parts, and only a single narrow global minimum. If we are only allowed to probe this loss landscape one value at a time we will have really hard time reaching the global minimum. However, if we have additional information we might be able to do better. . Let&#39;s assume that we know $ Delta^{-1} L( theta)$ where $ Delta$ is the Laplace operator. Physics interpretation is the following. If we view the loss landscape as the charge density $ Delta^{-1}L( theta)$ is the corresponding electric potential, sketched in red. Minimizing the electric potential instead of the charge density might be a much nicer problem because the electric field (the gradient of the potential field) typically stretches far away from localized charges and can attract the probe. Extreme example is the charge density of the point particle, which is impossible to find unless you trip over it. However, if you can probe the electric field of this charge you have an easy way discovering where it comes from. Sounds good, right? Well, not so fast. First, we do not know $ Delta^{-1}L( theta)$ for a typical VQE loss. Second, my examples were specifically crafted to sell the idea. It is easy to imagine a loss landscape where this does not help. . But we are not boring nitpickers, are we? Of course not, we are imaginative and brave, so we are going to assume even more. Let&#39;s pretend that each successive application of $ Delta^{-1}$ makes our loss landscape better, so we are really interested in begin{align*} mathcal{L}( theta)= Delta^{- infty} L( theta) . end{align*} Turns out this limiting landscape is very simple and can be found in a similar way to the average considered in the previous section. Indeed, the loss function eqref{loss exp} can be represented in the following form begin{align*} L( theta)=const+A_1 cos( theta_1)+B_1 sin( theta_1)+A_2 cos( theta_2)+B_2 sin( theta_2)+ A_{12} cos( theta_1) cos( theta_2)+B_{12} cos( theta_1) sin( theta_2)+ dots end{align*} This is an example with two parameters and several cross-terms are omitted. Here is the key point -- each cross-term gets smaller under application of $ Delta^{-1}$, e.g. $ Delta^{-1} cos theta_1= cos theta_1$, $ Delta^{-1} cos theta_1 cos theta_2= frac12 cos theta_1 cos theta_2$ etc. This means, that under the application of $ Delta^{- infty}$ only the single-variable terms will survive (we ignore the constant term) begin{multline*} mathcal{L}( theta)= Delta^{- infty}L( theta)= sum_{i=1}^{p} left(A_i cos theta_i+B_i sin theta_i right) substack{p=2 =} , ,A_1 cos( theta_1)+B_1 sin( theta_1)+A_2 cos( theta_2)+B_2 sin( theta_2) end{multline*} . These terms are easy to compute if we can compute averages. For example, averaging over all $ theta_i$ except theta $ theta_1$ will only leave the monomials with $ theta_1$ begin{align*} A_1 cos{ theta_1}+B_1 sin{ theta_1}= frac{1}{(2 pi)^{p-1}} int_0^{2 pi} prod_{i=2}^pd theta_i , , L( theta) . end{align*} How to perform the average on the rhs was shown in the previous section. Since the limiting loss function $ mathcal{L}( theta)$ is a sum of single-variable terms it is extremely simple to optimize, and the minumimum is unique . begin{align*} theta^* = ( theta_1^*, theta_2^*, dots) = operatorname{argmin}_ theta mathcal{L}( theta) . end{align*}The derivation above also gives a different, less exotic, interpretation of $ theta^*$. Each angle $ theta_i^*$ is the angle that minimizes the average loss function, where average is taken with respect to all other angles. Knowing how to find parameter value that optimizes an average performance does not seem like a completely useless information, does it? Could it help to alleviate the problems with barren plateaus or local minimums? I do not know, but I&#39;m planning on making some numerical experiments along these lines. . Beyond simple trigonometry . I was really impressed with a recent paper Beyond Barren Plateaus: Quantum Variational Algorithms Are Swamped With Traps. Adopting some techniques from the study of neural networks the authors provide a random matrix theory description of a generic loss landscape for Hamiltonian-agnostic VQE. They show that for underparametrized circuits (when the number of parameters $p$ is smaller than the dimension of the unitary group $4^n$, which is basically the only reasonable setup) the loss landscape is really bad, with exponentially many local minimums being located far away (energy-wise) from the global minimum. They even provide an analytic distribution for the expected number of local minimums, which seems to match my numerical experiments rather well: . This is yet another piece of evidence showing that complexity of the classical optimization loop in variational algorithms can not be ignored, as if the problems posed by getting a real quantum device to work were not enough. One possible way to alleviate the issues is to use specifically designed parametrized circuits, which are aware of the symmetries or additional properties of the Hamiltonian. I have a feeling though, that even performance of a generic Hamiltonian-agnostic VQE could be improved by exploiting some structural properties of the loss landscapes. In this blog post I speculated about what could such an approach look like. .",
            "url": "https://idnm.github.io/blog/blog/qml/vqa/vqe/2022/05/17/Some-analytic-facts-about-variational-algorithms.html",
            "relUrl": "/qml/vqa/vqe/2022/05/17/Some-analytic-facts-about-variational-algorithms.html",
            "date": " • May 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Machine learning compilation of quantum circuits -- experiments",
            "content": "Introduction . # If you are running this notebook in Colab, you might need to restart # the environment after the installations. from functools import partial import matplotlib.pyplot as plt import numpy as np import jax.numpy as jnp from jax import random, value_and_grad, jit, vmap, grad, lax from scipy.stats import unitary_group try: import optax except ImportError: !pip install optax import optax try: import qiskit except ImportError: !pip install qiskit !pip install pylatexenc # required for circuit drawing. import qiskit from qiskit import QuantumCircuit, transpile from qiskit.quantum_info import Operator, Statevector from qiskit.circuit import Parameter from qiskit.transpiler.passes.synthesis import UnitarySynthesis from qiskit.transpiler import PassManager from qiskit.converters import circuit_to_gate . . Motivation . Ever since I read the paper by L.Madden and A.Simonetto (original preprint, my review) I knew I want to do this kind of experiments myself. At first I hoped that there is a well-developed software framework where I can easily build quantum circuits and then optimize them efficiently. However, I was not able to find a good fit for my problem. For example, to the best of my knowledge qiskit currently only provides acess to zero-order optimization routines. I later found quimb which might do what I want, but in the end I&#39;m glad I worked things out from scratch. Eventually I went for numpy+JAX combination which while being quite low-level was not a big problem to get working and shows a decent speed. I owe a ton to Ilia Luchnikov for introducing me to the framework and helping throught. . In this post I will give a walk thorough this implementation and show experiments with compilation of random unitaries. However, in my opinion truly interesting stuff is concerned with the compilation of special gates, say multi-controlled Toffolis on restricted connectivity. I intend to look at this kind problems in detail in a future blog post. You may wish to take a look at this preprint for advances in that direction. . NOTE:While I was working on my experiments another preprint appeared, by P.Rakyta and Z.Zimborás, which is very similar to the work of M&amp;S in terms of numerical results. Despite the striking similarities these works are independent. As a bonus R&amp;Z also provide a numerical package SQUANDER that allows to play with their framework for compilation of unitaries. You might want to check that out if you are interested in doing some experiments yourself. . The problem . OK, so first a brief recap of what is the compilation problem. Given a quantum circuit we need to find an equivalent one, which satisfies certain requirements. A typical restrictions are to use only some specific two-qubits gates and to be compatible with limited connectivity. I gave a more detailed intro here. Here is a nearly-trivial example: a simple $CNOT$ gate . qc = QuantumCircuit(2) qc.cx(0, 1) qc.draw(output=&#39;mpl&#39;) . . can be decomposed in terms of the entangling $cz$ gate and single-qubit gates $rx, ry, rz$ as follows . qc_compiled = transpile(qc, basis_gates=[&#39;cz&#39;, &#39;rx&#39;, &#39;ry&#39;, &#39;rz&#39;], optimization_level=3) qc_compiled.draw(output=&#39;mpl&#39;) . . Now, for generic $n$-qubit unitaries one needs exponentially many entangling gates for the compilation. More precisely, there is a theoretical lower bound $ #CNOTs ge frac14 left(4^n-3n-1 right)$ on the amount of $CNOT$s required for compilation of any $n-$qubit unitary outside a measure zero set. Crucially, this measure zero set might in fact be of principal interest to quantum computing as it includes many operators featuring in most algorithms (such as multi-controlled gates). In this post I will only adress compilation of random unitaries and discuss compilation of special cases in a future post. For later reference here is the function computing the theoretical lower bound. . def TLB(n): return int((4**n-3*n-1)/4 + 1) for n in range(1, 7): print(&#39;TLB for {}-qubit unitary is {}&#39;.format(n, TLB(n))) . TLB for 1-qubit unitary is 1 TLB for 2-qubit unitary is 3 TLB for 3-qubit unitary is 14 TLB for 4-qubit unitary is 61 TLB for 5-qubit unitary is 253 TLB for 6-qubit unitary is 1020 . Now, there is an algorithm called quantum Shannon decomposition to decompose an arbitary $n$-qubit unitary into a sequence of $CNOT$s and single-qubit rotations which requires roughly twice as many $CNOT$s as the theoretical lower bound implies. In complexity-theoretic terms this is definitely good enoough, the overhead is just a small constant factor. However, for NISQ devices doubling the amount of gates is not a trivial matter. Is it possible to do better? . 3-qubit example . As papers M&amp;S and R&amp;Z show, one can do better and eliminate the 2x overhead, at least numerically. Namely, it seems that precisely at the theoretical lower bound the exact or nearly-exact compilation of any unitary is possible. Here is a real-life example. Consider the following 3-qubit circuit with $TLB(3)=14$ $CNOT$ gates . . The claim is that with the appropriate choice of angles in rotation gates it can morhp into any 3-qubit unitary (and in fact at least this many $CNOT$s are needed for almost all 3-qubit unitaries). To find the corresponding angles it is sufficient to run a numerical optimization minimizing the fidelity between this circuit&#39;s unitary and the target unitary. To me this is rather imressive, but raises several questions. Why choose $CNOT$ gates of all entangling gates? Why place them in that exact order as shown at the figure? It appears to be an empirical fact that precise location of entangling gates as well as their choice ($CNOT$, $cz$, etc) makes little difference. Moreover, even restricted connectivity does not seem to force an overhead for compilation. It is my main goal to back up these claims with numerical experiments in an interactive way. In particular, I will illustrate the following points. . Exactly at the theoretical lower bound a nearly-exact compilation seems to always be possible (at least for up to 6 qubits). This is a 2x improvement over the best theoretical decomposition. | Both $cz$ and $CNOT$ gates perform equally well. It is tempting to guess that any entangling gate will perform similarly. | The maximum fidelity is a monotonic function of the number of entangling gates. This implies that simply counting 2-qubit gates gives a good measure of circuits expressivity. | The most remarkable for me is the fact that even a restricted topology seems to cause no overhead on compilation cost. I will show that even on a chain topology the same amount of $CNOT$s is sufficient to reach good fidelity. | What you&#39;ll find if you keep reading . The rest of this post is divided into two parts. In the first I write some numpy/JAX/qiskit code that allows to construct and efficiently optimize parametrized circuits. I try to give some explanations of the underlying numerical framework, but please take into account that my own understanding is rather limited. Still, the resulting performance seems to be good enough to reproduce results of the existing preprints. I advise to skip this part if you are only interested in the results. . In the second part of the post I will do a number of experiments compiling random unitaries with varying numbers of qubits, different types of entangling gates, restricted connectivity and try to draw some general lessons from them. I tried to make this part independent of the first, although I didn&#39;t stop all the implementation details from sinking trough. . NOTE:This blog post is also a fully functional jupyter notebook. You can open it in Colab or download locally and perform more experiments yourself! . Numerical framework . Entangling blocks . First let us define the basic 1- and 2-qubit gates in matrix form. For now you can safely ignore the use jnp arrays instead of np arrays. . # Controlled-NOT (or controlled-X gate) cx_mat = jnp.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]) # Controlled-Z gate cz_mat = jnp.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, -1]]) # Pauli matrices x_mat = jnp.array([[0, 1], [1, 0]]) y_mat = jnp.array([[0, -1j], [1j, 0]], dtype=jnp.complex64) z_mat = jnp.array([[1, 0], [0, -1]]) # Rotation gates def rx_mat(a): return jnp.cos(a/2)*jnp.identity(2)-1j*x_mat*jnp.sin(a/2) def ry_mat(a): return jnp.cos(a/2)*jnp.identity(2)-1j*y_mat*jnp.sin(a/2) def rz_mat(a): return jnp.cos(a/2)*jnp.identity(2)-1j*z_mat*jnp.sin(a/2) . WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) . The circuits that we are going to train will be built out of two types of 2-qubit blocks, the controlled-Z and the controlled-NOT. Here are the definitions: . class block(): &quot;&quot;&quot;Two-qubit entangling block. Methods: circuit: gives equivalent `qiskit` circuit. unitary: gives `jax.numpy` unitary matrix of the circuit. &quot;&quot;&quot; def __init__(self, gate_name, angles): self.gate_name = gate_name self.angles = angles def circuit(self): &quot;&quot;&quot;Quantum circuit in `qiskit` corresponding to our block.&quot;&quot;&quot; qc = QuantumCircuit(2) if self.gate_name == &#39;cx&#39;: qc.cx(0, 1) elif self.gate_name == &#39;cz&#39;: qc.cz(0, 1) else: print(&quot;Gate &#39;{}&#39; not yet supported&#39;&quot;.format(self.gate_name)) angles = np.array(self.angles) # convert from JAX array to numpy array if applicable. qc.ry(angles[0], 0) qc.rx(angles[1], 0) qc.ry(angles[2], 1) qc.rx(angles[3], 1) return qc def unitary(self): &quot;&quot;&quot;JAX-compatible unitary corresponding to our block.&quot;&quot;&quot; if self.gate_name == &#39;cx&#39;: entangling_matrix = cx_mat elif self.gate_name == &#39;cz&#39;: entangling_matrix = cz_mat else: print(&quot;Gate &#39;{}&#39; not yet supported&#39;&quot;.format(self.gate_name)) x_rotations = jnp.kron(rx_mat(self.angles[1]), rx_mat(self.angles[3])) y_rotations = jnp.kron(ry_mat(self.angles[0]), ry_mat(self.angles[2])) return x_rotations @ y_rotations @ entangling_matrix . Here is how they look: cz block . a0, a1, a2, a3 = [Parameter(a) for a in [&#39;a0&#39;, &#39;a1&#39;, &#39;a2&#39;, &#39;a3&#39;]] block(&#39;cz&#39;, [a0, a1, a2, a3]).circuit().draw(output=&#39;mpl&#39;) . and cx block . block(&#39;cx&#39;, [a0, a1, a2, a3]).circuit().draw(output=&#39;mpl&#39;) . Our block class can return a qiskit circuit and the corresponding unitary matrix. Of course we could have extracted the unitary from the circuit itself via qiskit API, but this would make the matrix representation incompatible with JAX which will be our workhorse for optimization. To the best of my knowledge currently it is only possible to use zero-order methods directly from qiskit which is a serious limitation. So at this point we needed a bit of wheel reinvention. Let&#39;s check that our implementation is consistent with qiskit: . angles = random.uniform(random.PRNGKey(0), shape=(4,), minval=0, maxval=2*jnp.pi) for gate in [&#39;cx&#39;, &#39;cz&#39;]: b = block(gate, angles) qc = b.circuit() qs_unitary = Operator(qc.reverse_bits()).data # Yes, we need to reverse bits in qiskit to match our conventions. our_unitary = b.unitary() print(&#39;qiskit unitary is the same as our unitary for block with gate {}: {}&#39;.format(gate, jnp.allclose(qs_unitary, our_unitary))) . qiskit unitary is the same as our unitary for block with gate cx: True qiskit unitary is the same as our unitary for block with gate cz: True . To match matrix representations of quantum circuits might be a headache as I discussed in another post, so this was a necessary check to do. . Our two building blocks (cz and cx) only differ by the type of the two-qubit gate. The circuits that we are going to build seem to do equally well for any choice of two-qubit gate. I will mostly use cz gate because it is symmetric under the swap of qubits, but I will also occasionally bring up the cx gate to illustrate that it has the same performance. Angles $a_0$-$a_3$ are going to be optimized. . Optimization with JAX . A word about JAX . What is JAX? Well, I personally think of it as numpy on steroids. You can check out the official documentation or numerous nice overwievs on the web. For our purposes two key features of JAX are . Autograd. | JIT or just-in-time compilation. | Autograd allows to define functions the same way you do in numpy and have analytic derivatives available with no extra coding on your side. At the moment grad function can only be applied to real scalars. For example, let us define the absolute value of the trace of cx block as function of rotations gate angles . def block_tr_abs(angles): b = block(&#39;cx&#39;, angles) tr = jnp.trace(b.unitary()) return jnp.abs(tr) . Since everything so far has been defined using jax.numpy we have immediate access to the gradient of this function . grad(block_tr_abs)([0.,1.,2.,3.]) . [DeviceArray(0.03655498, dtype=float32), DeviceArray(-0.25903472, dtype=float32), DeviceArray(-0.7384602, dtype=float32), DeviceArray(-7.450581e-09, dtype=float32)] . Autograd feature of JAX allows us to just define the loss function associated with our circuit in plain numpy terms and use advanced first-order optimizers such as Adam out of the box. . The next crucial ingredient is jit-compilation. When used with a bit of care, it allows to speed up evaluation of similar expression by orders of magnitude. For example let us compare runtimes of the jitted and unjitted versions of our trace function. Let&#39;s first define a sample of random angles . test_angles = random.uniform(random.PRNGKey(0), shape=(1000, 4), minval=0, maxval=2*jnp.pi) . and now time evaluation of unjitted trace function . %%time for angles in test_angles: block_tr_abs(angles) . CPU times: user 12.3 s, sys: 1.02 s, total: 13.3 s Wall time: 11.4 s . Now awe to the power of jit! . %%time jit_block_tr_abs = jit(block_tr_abs) for angles in test_angles: jit_block_tr_abs(angles) . CPU times: user 156 ms, sys: 7.94 ms, total: 164 ms Wall time: 145 ms . What happened here is that during the first call to the jitted function it&#39;s efficient XLA version was compiled and then used to evaluate all subsequent calls. . Gradient descent . We will use the following measure of discrepancy between two unitaries $disc(U, V) = 1- frac1{N} operatorname{Tr} left( U^ dagger V right)$ where $U,V$ are $N times N$ matrices. It is normalized so that $disc(U,U)=0$ and $disc(U,V)=0$ when $U$ and $V$ are orthogonal. Note that this measure is insensitive to global phases. . def disc(U, U_target): n = U_target.shape[0] return 1-jnp.abs((U.conj() * U_target).sum())/n . Here is the optimization routine that we are going to use. It is pretty straightforward and I will not give much explanations, but illustrate with an example. . @partial(jit, static_argnums=(0, 1, )) # &lt; Here is where the magic happens! # Remove this line and everything will run 1000 times slower:) def unitary_update(loss_and_grad, opt, opt_state, angles): &quot;&quot;&quot;Single update step.&quot;&quot;&quot; loss, grads = loss_and_grad(angles) updates, opt_state = opt.update(grads, opt_state) angles = optax.apply_updates(angles, updates) return angles, opt_state, loss def unitary_learn(U_func, U_target, n_angles, init_angles=None, key=random.PRNGKey(0), learning_rate=0.01, num_iterations=5000, target_disc=1e-10): &quot;&quot;&quot;Use Adam optimizer to minimize discrepancy between pamaterzied unitary and targe unitary. Args: U_func: function of angles returning univary matrix. U_target: unitary matrix to approximate. n_angles: total number of angles (parameters) in U_func. init_angles: intial angles for gradient descent. If not provided chosen at random. key: random seed to use for inizialization of initial angles. learning_rate: learning rate in Adam optimizer. num_iterations: maximum number of iterations. target_disc: stop optimization if discrepancy drops below target_disc. Returns: tuple (angles_history, loss_history) where angles_history: list of angles (parameters) at each iteration step. loss_history: values of loss_function at each iteration step. &quot;&quot;&quot; # If initial angles are not provided generate them at random. if init_angles is None: key = random.PRNGKey(0) angles = random.uniform(key, shape=(n_angles,), minval=0, maxval=2*jnp.pi) else: angles = init_angles # Loss function to minimize is dicrepancy defined above. loss_func = lambda angles: disc(U_func(angles), U_target) loss_and_grad = value_and_grad(loss_func) # Optimizer is taken from the `optax` library and its use is self-explanotory. opt = optax.adam(learning_rate) opt_state = opt.init(angles) # Optimization cycle angles_history=[] loss_history=[] for _ in range(num_iterations): angles, opt_state, loss = unitary_update(loss_and_grad, opt, opt_state, angles) angles_history.append(angles) loss_history.append(loss) if loss &lt; target_disc: break return angles_history, loss_history . OK, now a very simple example. Say we want to find a $ZXZ$ decomposition of $Y$-gate. Define: . def zxz_ansatz(angles): return rz_mat(angles[0]) @ rx_mat(angles[1]) @ rz_mat(angles[2]) . Learning is now very simple: we give unitary_learn the ansatz unitary as function of angles, the target unitary and also explicitly the number of parameters to be trained: . angles_history, loss_history = unitary_learn(zxz_ansatz, y_mat, 3) . We can visualize the learning progress as follows: . plt.plot(loss_history) plt.yscale(&#39;log&#39;) . The learned angles in $ZXZ$ decomposition are . angles_history[-1] . DeviceArray([6.59216 , 3.1411407, 3.4505684], dtype=float32) . It is not difficult to check directly that the result is equal to the $Y$ matrix up to a global phase with reasonable accuracy, indeed . jnp.around(1j*zxz_ansatz(angles_history[-1]), 3) . DeviceArray([[0.+0.j, 0.-1.j], [0.+1.j, 0.+0.j]], dtype=complex64) . Quantum circuits with numpy . Now it&#39;s time to build full quantum circuits. We will think of a quantum circuit on $n$ qubits as a tensor with $2*n$ legs. First $n$ legs correspond to output and last to $n$ input. This is illustrated at the picture. . . It is natural for input legs to be on the left because in matrix notation a unitary $U$ acts on a state $ psi$ by left multiplication $U psi$. On the other hand note that quantum circuits are usually drawn left-to-right and to compare the two descriptions a left-right reflection must be made. . Suppose now that given an $n-$qubit circuit $U$ we want to append an additional $m-$qubit gate $V$ at the end. Here is a concrete example (a picture is worth a thousand words!) . Several things to keep in mind: . To append gate $V$ at the end in quantum circuit notation, we need to draw it on the left here. | Tensor legs are joined by numpy&#39;s tensordot operation. Which axes to contract is clear from the picture -- we need to join axes 2, 3 of $V$ to 1, 3 of $U$. | In the resulting tensor the output legs are not in the correct order. Instead of being numbered from top to bottom after tensordot first several axes are those of $V$ and the remaining are uncontracted output axes of $U$ (take a look at the leftmost column of numbers). This needs to be corrected by explicit transposition of output axes. | The final caveat is that if some of the legs connecting gate to the circuit are twisted the output legs needs to be transposed accordingly. Here is an example | . Here is the code that implements this program. . def gate_transposition(placement): &quot;&quot;&quot;Determine transposition associated with initial placement of gate.&quot;&quot;&quot; position_index = [(placement[i], i) for i in range(len(placement))] position_index.sort() transposition = [i for _,i in position_index] return transposition def transposition(n_qubits, placement): &quot;&quot;&quot;Return a transposition that relabels tensor axes correctly. Example (from the figure above): n=6, placement=[1, 3] gives [2, 0, 3, 1, 4, 5]. Twiseted: n=6, placement=[3, 1] gives [2, 1, 3, 0, 4, 5].&quot;&quot;&quot; gate_width = len(placement) t = list(range(gate_width, n_qubits)) for position, insertion in zip(sorted(placement), gate_transposition(placement)): t.insert(position, insertion) return t def apply_gate_to_tensor(gate, tensor, placement): &quot;&quot;&quot;Append `gate` to `tensor` along legs specified by `placement`. Transpose the output axes properly.&quot;&quot;&quot; gate_width = int(len(gate.shape)/2) tensor_width = int(len(tensor.shape)/2) # contraction axes for `tensor` are input axes (=last half of all axes) gate_contraction_axes = list(range(gate_width, 2*gate_width)) contraction = jnp.tensordot(gate, tensor, axes=[gate_contraction_axes, placement]) # input(=last half) indices are intact t = transposition(tensor_width, placement) + list(range(tensor_width, 2*tensor_width)) return jnp.transpose(contraction, axes=t) . Now, using this tensor language we will construct unitary matrices corresponding to our ansatz circuits. To specify the ansatz we must supply the number of qubits in the circuit, type of entangling blocks to use and arrangement of these blocks. . The simplest way to specify arrangement would be to just give a list like [[0,1], [1, 3], [2, 1]] etc of pairs of qubits to put entangling blocks on to. However for performance reasons I need to make it more complicated. To construct a matrix for our quantum circuit we basically need to loop over all entangling gates and append them one by one. When using JAX plain python loops are simply unrolled and then compiled. For large loops this leads to very large compilation times. If there is no structure in how we place our gates in the circuit this is probably the best one can do. However, we can be more efficient than that if there is a structure. Take a look at this picture . qc = QuantumCircuit(4) i = 0 for _ in range(11): qc.cx(i,i+1) i = (i+1) % 3 if i % 3 == 0: qc.barrier() qc.draw() . . ░ ░ ░ q_0: ──■─────────────░───■─────────────░───■─────────────░───■─────── ┌─┴─┐ ░ ┌─┴─┐ ░ ┌─┴─┐ ░ ┌─┴─┐ q_1: ┤ X ├──■────────░─┤ X ├──■────────░─┤ X ├──■────────░─┤ X ├──■── └───┘┌─┴─┐ ░ └───┘┌─┴─┐ ░ └───┘┌─┴─┐ ░ └───┘┌─┴─┐ q_2: ─────┤ X ├──■───░──────┤ X ├──■───░──────┤ X ├──■───░──────┤ X ├ └───┘┌─┴─┐ ░ └───┘┌─┴─┐ ░ └───┘┌─┴─┐ ░ └───┘ q_3: ──────────┤ X ├─░───────────┤ X ├─░───────────┤ X ├─░─────────── └───┘ ░ └───┘ ░ └───┘ ░ . Here $CNOT$s are just placeholders for any entangling block of our interest. There is a regular pattern. Most of the circuit consists of identical layers up to a couple of final gates. Construction and optimization of such circuits with JAX can be made way more efficient by using lax.fori_loop (see here for docs) or a similar construct. This allows to exploit the regularity and reduce the compilation time dramatically. . The price to pay is a bit of a hassle in separating all gates into regular ones and the remainder. My core function build_unitary accepts the regular layers as an argument layer_placements=[layer, number_of_repetitions] and the remainder gates are described by free_placements. Also, we need some way to access all parameters (angles) in our circuit. I chose the simplest approach here, to supply angles as a 1d array, but internally they play a bit different roles so there is also a function split_angles to separate a 1d array of all angles into several logical blocks. . OK, so here is the code. Examples are found in the end of this section. . def split_angles(angles, num_qubits, layer_len, num_layers, free_placements_len): &quot;&quot;&quot;Splits 1d array of all angles in a circuit into four groups. Args: angles: all angles in a circuit as 1d array. num_qubits: number of qubits in a circuit. layer_len: length (depth) of a single layer in a circuit. num_layers: number of repeated layers. free_placements_len: number of entanglig blocks not in layers. Returns: a tuple (surface_angles, layers_angles, free_block_angles) where surface_angles: angles in initial single-qubit blocks. block_angles: angles of all entangling blocks. layers_angles: angles for entangling blocks that are parts of complete layers. free_block_angles: angles of remaining entangling blocks. &quot;&quot;&quot; surface_angles = angles[:3*num_qubits].reshape(num_qubits, 3) block_angles = angles[3*num_qubits:].reshape(-1, 4) layers_angles = block_angles[:layer_len*num_layers].reshape(num_layers, layer_len, 4) free_block_angles = block_angles[layer_len*num_layers:] return surface_angles, block_angles, layers_angles, free_block_angles def build_unitary(num_qubits, block_type, angles, layer_placements=((), 0), free_placements=()): &quot;&quot;&quot; Builds `JAX`-compatible unitary matrix of a quantum circuit. Arguments specify structure of the circuit and values of parameters. Args: num_qubits: number of qubits. block_type: type of entangling block to use. Currently only &#39;cx&#39; and &#39;cz&#39; are supported. angles: 1d array of all angle parameters in the circuit. layer_placements: a tuple (single_layer, n) where `single_layer` specifies positions of several entangling blocks and `n` how many time to repeat each layer. free_placements: Positions of entangling blocks that do no belong to layers. Returns: A `jax.numpy` unitary matrix of the quantum circuit. &quot;&quot;&quot; layer, num_layers = layer_placements layer_depth = len(layer) num_blocks = len(layer)*num_layers+len(free_placements) # Count all entangling blocks. # Divides 1d array of all angles into three logically distinct groups. surface_angles, _, layers_angles, free_block_angles = split_angles(angles, num_qubits, len(layer), num_layers, len(free_placements)) # Initizlizes identity matrix of the proper size. u = jnp.identity(2**num_qubits).reshape([2]*num_qubits*2) # Unitary matrix is built in three steps. # First, 3 single-qubit gates are applied to each qubit. # Second, all entangling blocks that are parts of layers are applied. # Finally, remainder blocks that a not parts any layer are applied. # Initial round of single-qubit gates for i, a in enumerate(surface_angles): gate = rz_mat(a[2]) @ rx_mat(a[1]) @ rz_mat(a[0]) u = apply_gate_to_tensor(gate, u, [i]) # Sequence of layers wrapped in `fori_loop`. # Using `fori_loop` instead of plain `for` loop reduces the compilation time significantly. # To use `fori_loop` it is convenient to define a separate function that applies a whole layer of gates. def apply_layer(i, u, layer, layers_angles): &quot;&quot;&quot;Apply several gates to a given quantum circuit. Supplying the totality of `layers_angles` makes the function compatible with `fori_loop`. Args: i: index of the layer. u: matrix to apply gates to. layer: positions of all gates to be applied. layers_angles: angles of all layers. &quot;&quot;&quot; layer_angles = layers_angles[i] for block_angles, position in zip(layer_angles, layer): gate = block(block_type, block_angles).unitary().reshape(2,2,2,2) u = apply_gate_to_tensor(gate, u, position) return u if num_layers&gt;0: u = lax.fori_loop(0, num_layers, lambda i, u: apply_layer(i, u, layer, layers_angles), u) # Adds the remainding (free) entangling blocks. for angles, position in zip(free_block_angles, free_placements): gate = block(block_type, angles).unitary().reshape(2,2,2,2) u = apply_gate_to_tensor(gate, u, position) return u.reshape(2**num_qubits, 2**num_qubits) . Layers . Here are a couple of simple functions to help define gate arrangements. The basic layer is sequ_layer which consists of entangling gates applied to each possible pair of two qubit gates enumerated by pairs $(i,j)$ with $i&lt;j$. . def sequ_layer(num_qubits): return [[i,j] for i in range(num_qubits) for j in range(i+1, num_qubits)] def fill_layers(layer, depth): num_complete_layers = depth // len(layer) complete_layers = [layer, num_complete_layers] incomplete_layer = layer[:depth % len(layer)] return complete_layers, incomplete_layer . Function fill_layers allows to specify how much entangling gates we want in total and splits them into complete layers (to be used as layer_placements) and possible remainder gates (that become free_placements). For example, a sequ_layer on three qubits consists of three gates at positions . sequ_layer(3) . [[0, 1], [0, 2], [1, 2]] . If we want to have the sequ pattern and 10 entangling gates in total we can put three complete layers and a final single gate. fill_layers does just that . layer_placements, free_placements = fill_layers(sequ_layer(3), 10) print(layer_placements) print(free_placements) . [[[0, 1], [0, 2], [1, 2]], 3] [[0, 1]] . Packing everything together: ansatz circuits . Now that we have defined our building blocks and convenience functions to assemble them it is time to pack everything together and reap the harvest. . I will define ansatz class that assembles our building blocks according to a predefined pattern. It&#39;s circuit method gives a qiskit circuit which can be used for visualization and cross-checks. It&#39;s unitary attribute returns fully jax-compatible matrix representation of the same circuit. Finally, its learn method uses our optimization routine to approximate a target unitary. First the code, then an example. . class Ansatz(): &quot;&quot;&quot;Parametric quantum circuit. Ansatz/parametric circuit is defined by tupes of entangling blocks and their arrangement. Concrete values of parameters are not considered part of the ansatz. Class provides access to both `qiskit` version of the circuit and `jax.numpy` unitary matrix. Attributes: num_qubits: number of qubits block_type: type of entangling blocks num_angles: total number of angles (parameters) in the circuit. unitary: `jax.numpy` unitary matrix of the circuit as function of angles. Methods: circuit: `qiskit` version of the circuit. learn: numerical approximation of the target unitary. &quot;&quot;&quot; def __init__(self, num_qubits, block_type, layer_placements=[[], 0], free_placements=[]): self.num_qubits = num_qubits self.block_type = block_type self.layer, self.num_layers = layer_placements self.free_placements = free_placements self.all_placements = self.layer*self.num_layers+free_placements self.num_angles = 3*num_qubits+4*len(self.all_placements) self.unitary = lambda angles: build_unitary(self.num_qubits, self.block_type, angles, layer_placements=[self.layer, self.num_layers], free_placements=self.free_placements) def circuit(self, angles=None): &quot;&quot;&quot;qiskit version circuit. If angles not specified a parametric circuit is constructed.&quot;&quot;&quot; if angles is None: angles = np.array([Parameter(&#39;a{}&#39;.format(i)) for i in range(self.num_angles)]) surface_angles, block_angles, _, _ = split_angles(angles, self.num_qubits, len(self.layer), self.num_layers, len(self.free_placements)) qc = QuantumCircuit(self.num_qubits) # Initial round of single-qubit gates. for n, a in enumerate(surface_angles): qc.rz(a[0], n) qc.rx(a[1], n) qc.rz(a[2], n) # Entangling gates accoring to placements for a, p in zip(block_angles, self.all_placements): qc_block = block(self.block_type, a).circuit() qc = qc.compose(qc_block, p) return qc def learn(self, u_target, **kwargs): &quot;&quot;&quot;Use numerical optimization to approximate u_target.&quot;&quot;&quot; u_func = self.unitary return unitary_learn(u_func, u_target, self.num_angles, **kwargs) . Here is an example that should illustrate how all this can be used. . n_qubits = 3 block_type = &#39;cx&#39; # For technical reasons all entangling gates are divided into &#39;layers&#39; and &#39;free&#39; gates. single_layer = [[0, 1], [2, 1]] # We make single layer consisting of &#39;cx&#39; block on qubits [0,1] # followed by reversed &#39;cx&#39; block on qubits [1,2]. layers = [single_layer, 3] # The layer is repeated 3 times. free_placements = [[1, 0], [0, 1], [1, 2], [2, 1]] # Apeend remaining `free placements` a. anz = Ansatz(n_qubits, block_type, layer_placements=layers, free_placements=free_placements) . Here is what resulting circuit looks like. . anz.circuit().draw(output=&#39;mpl&#39;) . Just to make sure let us check that the unitary matrix of this circuit extracted from qiskit agrees with our own implementation for a random set of angles. . angles = random.uniform(random.PRNGKey(0), shape=(anz.num_angles,), minval=0,maxval=2*jnp.pi) qs_u = Operator(anz.circuit(angles).reverse_bits()).data # qiskit matrix representation our_u = anz.unitary(angles) # our matrix representation print(jnp.allclose(qs_u, our_u, rtol=1e-6, atol=1e-7)) . True . Experiments . Now that the hard work is behind we can sit back and reap the benefits. I will go through a series of examples. Primary goal is to back up the claims from the introduction about reaching the theoretical lower bound, agile performance on restricted topology etc. But I will also try to make clear how my code can be used if you wish to do a little experimenting with it yourself. . Learning 2-qubit random unitary . Let&#39;s start by learning a random 2-qubits unitary. First, define one. . u_target = unitary_group.rvs(4, random_state=0) . Here is the parametrized circuit we are going to use. cz means that the entangling gate is controlled-Z while free_placements are just positions where to put these entangling gates. There isn&#39;t much choice for 2 qubits as you could guess. I will explain why I call these free_placements a bit later. . anz = Ansatz(2, &#39;cz&#39;, free_placements=[[0,1], [0,1], [0, 1]]) anz.circuit().draw(output=&#39;mpl&#39;) # anz.circuit() is a fully-functional `qiskit` version of our ansatz. . The learning process is easy as pie: %%time angles_history, loss_history = anz.learn(u_target) plt.plot(loss_history) plt.yscale(&#39;log&#39;) . CPU times: user 2.45 s, sys: 21.4 ms, total: 2.48 s Wall time: 2.43 s . The graph shows that we achieve great fidelity in under 500 iterations. . Don&#39;t believe me? Is there a way to tell if this plot indeed reflects a successful compilation without looking under the hood? OK OK, since you&#39;re asking, I will double-check using pure qiskit: . angles = angles_history[-1] # Last(=best) angles in the optimization process. qc = anz.circuit(angles) # genuine qiskit circuit. u_qs = Operator(qc.reverse_bits()).data # qiskit API to extract the unitary matrix. disc(u_qs, u_target) # OK, I guess here you have believe I&#39;ve implemented the cost function properly. # If you want to compare the matrices component-wise, fine with me. . DeviceArray(2.3841858e-07, dtype=float32) . Similar checks can be done in more complicated scenarios below. . You can move forward to other examples or try some experiments here. Some ideas: . Changing gate type from cz to cx (should not affect the result). | Decreasing the number of layers (fidelity won&#39;t be nearly as good). | Increasing the number of layers (same fidelity with less iterations). | Learning 3-qubit random unitary . I advertised in the introduction that with just 14 entangling gates any 3-qubit unitary can be nearly perfectly approximated. Let me back up this claim. Here is how we can construct the corresponding ansatz. . num_qubits = 3 block_type = &#39;cz&#39; depth = 14 layer_placemets, free_placements = fill_layers(sequ_layer(num_qubits), depth) anz = Ansatz(num_qubits, block_type, layer_placements=layer_placements, free_placements=free_placements) anz.circuit().draw(output=&#39;mpl&#39;) . The way gate placements are passes to Ansatz here require a bit of unpacking. This is an implementation detail I didn&#39;t take enough care to hide. For technical reasons I explained in the numerical section optimization is much faster when gates are arranged in a regular pattern. The pattern we use here is called sequ_layer and for three qubits it is simply . sequ_layer(num_qubits) . [[0, 1], [0, 2], [1, 2]] . i.e. it just lists all possible pairs of three qubits. However, since 14 % 3 = 2 the two last gates do not fit into the regular pattern and require a bit of a special treatment. This is what the function fill_layers does for us. Indeed . layer_placements, free_placements = fill_layers(sequ_layer(num_qubits), depth) print(&#39;basic layer is repeated four times:&#39;, layer_placements) print(&#39;remaining blocks reside at positions:&#39;, free_placements) . basic layer is repeated four times: [[[0, 1], [0, 2], [1, 2]], 4] remaining blocks reside at positions: [[0, 1], [0, 2]] . I hope that did explain the way that gate positions are passed to the Ansatz. Instead of sequ_layer you can pass any arrangment of gates to be periodically repeated. We will do just that when considering a restricted topology. . Now let&#39;s run the optimization. . %%time u_target = unitary_group.rvs(2**num_qubits, random_state=0) angles_history, loss_history = anz.learn(u_target) plt.plot(loss_history) plt.yscale(&#39;log&#39;) . CPU times: user 9.75 s, sys: 177 ms, total: 9.92 s Wall time: 9.18 s . OK, I hope this does convince you that our ansatz was indeed good enough! Another interesting thing to do is to make a sweep to see how the fidelity increases (error drops) with the number of layers. . %%time best_loss = [[], []] for depth in range(15): # TLB(3)=14 layer_placemets, free_placements = fill_layers(sequ_layer(n_qubits), depth) for i, block_type in enumerate([&#39;cx&#39;, &#39;cz&#39;]): anz = Ansatz(num_qubits, block_type, layer_placements=layer_placemets, free_placements=free_placements) angles, loss_history = anz.learn(u_target, target_disc=10e-4) best_loss[i].append(min(loss_history)) plt.plot(best_loss[0], label=&#39;cx loss&#39;) plt.plot(best_loss[1], label=&#39;cz loss&#39;) plt.ylabel(&#39;error&#39;) plt.xlabel(&#39;number of entangling gates&#39;) plt.legend() . CPU times: user 3min 49s, sys: 6.68 s, total: 3min 55s Wall time: 3min 35s . &lt;matplotlib.legend.Legend at 0x7f39d2950a60&gt; . One lesson here is that both types of two-qubits gate perform similarly well at all depths. This is not surprising for because cx and cz gates can be related by single-qubit Hadamard transformations. It would be interesting to see if other two-qubit gates perform differently. . Another important observation is that the best fidelity is a monotonic function of the the amount of two-qubit gates. There is some work on variational algorithms testing various metrics that would adequately reflect expressivity of the ansatz. I think that plain number of $CNOT$ gates should in fact be a fantastic and simple metric for this. . Learning 6-qubit random unitary . I do know that 3 is followed by 4, but shall we perhaps get more ambitious? Let&#39;s try to compile a 6-qubit random unitary (you can try to go higher if your machine allows): . %%time num_qubits = 6 depth = TLB(num_qubits) # 1020 for 6 qubits layer_placements, free_placements = fill_layers(sequ_layer(num_qubits), depth) u_target = unitary_group.rvs(2**num_qubits, random_state=0) anz = Ansatz(num_qubits, &#39;cz&#39;, layer_placements=layer_placements, free_placements=free_placements) angles_history, loss_history = anz.learn(u_target, num_iterations=5000) plt.title(&#39;number of qubits: {}&#39;.format(num_qubits)) plt.xlabel(&#39;number of iterations&#39;) plt.ylabel(&#39;error&#39;) plt.plot(loss_history) plt.yscale(&#39;log&#39;) . CPU times: user 5min 33s, sys: 1min 1s, total: 6min 34s Wall time: 6min 28s . Note that depth of the theoretical lower bound for 6 qubits is $TLB(6)=1020$ which implies that there are $ approx 4000$ parameters in our ansatz. On my modest laptop the training completes in about 10 minutes. Of course I would not claim this to be the cutting edge, but our JAX setup seems to be competitive at the scale (3-6 qubits) addressed in the literature so far. . Restricted topology . One of the most remarkable features of this approach is that topology restrictions do not seem to bring any overhead to compilation of random unitaries. To make the point and illustrate this claim I will consider the least connected topology I can think of, the chain topology. The corresponding layer consists of all pairs of adjacent qubits. . def chain_layer(num_qubits): return [(i,i+1) for i in range(num_qubits-1)] . Here is a 6-qubit illustration. . Ansatz(6, &#39;cx&#39;, layer_placements=[chain_layer(6), 1]).circuit().draw(output=&#39;mpl&#39;) . Here I drew a single layer consisting of 5 blocks. To reach the theoretical lower bound requires to stack together 1020/5=204 layers. Let&#39;s do that and see how the learning goes. . %%time num_qubits = 6 depth = TLB(num_qubits) layer_placements, free_placements = fill_layers(chain_layer(num_qubits), depth) u_target = unitary_group.rvs(2**num_qubits, random_state=0) anz = Ansatz(num_qubits, &#39;cx&#39;, layer_placements=layer_placements, free_placements=free_placements) angles_history_chain, loss_history_chain = anz.learn(u_target) . CPU times: user 5min 9s, sys: 1min 3s, total: 6min 13s Wall time: 6min . Let&#39;s compare the results with the previously considered fully connected topology. . plt.title(&#39;number of qubits: {}&#39;.format(num_qubits)) plt.xlabel(&#39;number of iterations&#39;) plt.ylabel(&#39;error&#39;) plt.plot(loss_history, label=&#39;fully connected&#39;) plt.plot(loss_history_chain, label=&#39;chain&#39;) plt.legend() plt.yscale(&#39;log&#39;) . As you can see, the chain topology performs only slightly worse than the fully connected topology which seems truly remarkable. . Final remarks . The main goal was to illustrate that numerical compilation of small-scale random unitaries can be very efficient in terms of gate count, and seems to reach the theoretical lower bound in all cases considered, regardless of topological restrictions. . It is interesting to note that a variety of optimization procedures are used in the literature. In M&amp;S a simple version of the gradient descent is used, in R&amp;Z an interesting procedure of one-qubit gate decoupling is used (I must admit I do not understand exactly what it does), and in KTS preprint a funny optimization one-angle-at a time is used (because as a function of each angle the circuit is a simple triginometric function, it is trivial to optimize one parameter at a time). Here we used a slightly more advanced version of the gradient descent, the Adam algorithm. All approaches seem to work well on random unitaries. . My preliminary investigations show that for special gates things get much more complicated than for generic random unitaries. But this is where the most intersting stuff is found, e.g. compilation of multi-component Toffoli gates on restricted connectivity. I hope to address these cases in a future blog post! .",
            "url": "https://idnm.github.io/blog/blog/qiskit/jax/machine%20learning/compilation/2021/12/13/Machine-learning-compilation-of-quantum-circuits-experiments.html",
            "relUrl": "/qiskit/jax/machine%20learning/compilation/2021/12/13/Machine-learning-compilation-of-quantum-circuits-experiments.html",
            "date": " • Dec 13, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Matrix representation of quantum circuits - notations and gotchas",
            "content": "Intro . Usually, for experimenting with quantum circuits I use qiskit. As any higher level environment it is very convenient for common tasks, but may turn out too inflexible for unusual use cases. A somewhat opposite approach is to use much lower level tools to gain in flexibility at the expense of convenience. Currently I want to use Google&#39;s tensornetwork package for simulations and training of quantum circuits, but this requires building many things that are for free in qiskit from scratch. It is also necessary to become explicit about conventions for matrix representation of quantum circuits. As long as you stay within a single framework this may not be an issue. However for debugging purposes as well as for comparison between different frameworks this may become unavoidable. Thus, I always anticipated, that a day will come when I need to face my fears and order all terms in a tensor product by hands. Now it seems I&#39;m past the difficult part and I&#39;m better writing this down in case I would need to do something similar in the future. . Defining the problem . OK, so what is the problem? Consider the following simple circuit built with qiskit: . import numpy as np from qiskit import QuantumCircuit from qiskit.quantum_info import Operator, Statevector qc = QuantumCircuit(2) qc.x(0) qc.y(1) qc.cx(0,1) qc.draw(output=&#39;mpl&#39;) . . It is not hard or ambiguous to interpret what this circuit does by inspecting the diagram. Say the input state is $q_0=|0 rangle$, $q_1=|1 rangle$. After $X$ acts on $q_0$ it becomes $q_0 to X |0 rangle=|1 rangle$. Similarly, $q_1$ after $Y$ becomes $q_1 to Y|1 rangle=-i |0 rangle$. Since now $q_0$ is &quot;on&quot; the CNOT gate switches the state of $q_1$ further to $q_0 to -i|1 rangle$. So the end result is that $q_0=|0 rangle, q_1=|1 rangle$ is transformed to $q_0=|1 rangle, q_1=-i|1 rangle$. Or perhaps a picture says it better . . Similarly, we can work out what the circuit does for other computational basis states which by linearity fully fixes the action of the circuit. Although quite explicit, this is a clumsy description. This is why the matrix notation is usually used. And indeed, we can obtain the matrix corresponding to our quantum circuit quite easily in qiskit: . U_qs = Operator(qc).data U_qs . array([[0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j], [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j], [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j], [0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j]]) . It is important to realize that a number of conventions must be chosen before such explicit matrix representation can be written down. In particular, I will emphasize two points I tripped over while studying this: ordering of the qubit states in the tensor product or &quot;vertical ordering&quot; and ordering of operators or &quot;horizontal ordering&quot;. . . In the rest of the post I will clarify what are the conventions used in qiskit and how to reproduce the circuit with the tensornetwork library. . States: vertical ordering . Single qubit states . First we need to give matrix representations to two basis states of a single qubit. Here I think it is quite uncontroversial to choose begin{align} |0 rangle = begin{pmatrix}1 0 end{pmatrix}, qquad |1 rangle = begin{pmatrix}0 1 end{pmatrix} label{kets} end{align} . These are the &quot;ket&quot; vectors. Their &quot;bra&quot; counterparts are begin{align} langle 0| = begin{pmatrix}1 &amp; 0 end{pmatrix}, qquad langle 1| = begin{pmatrix}0 &amp; 1 end{pmatrix} label{bras} end{align} . With these, the following operators can be computed begin{align} |0 rangle langle 0| = begin{pmatrix}1 &amp; 0 0 &amp; 0 end{pmatrix}, qquad |0 rangle langle 1| = begin{pmatrix}0 &amp; 1 0 &amp; 0 end{pmatrix} nonumber |1 rangle langle 0| = begin{pmatrix}0 &amp; 0 1 &amp; 0 end{pmatrix}, qquad |1 rangle langle 1| = begin{pmatrix}0 &amp; 0 0 &amp; 1 end{pmatrix} label{ketbras} end{align} . Multiple qubit states . When there is more than a single qubit things become a bit more interesting and potentially confusing. For example, the combined Hilbert space of two qubits $ mathcal{H}_2$ is a tensor product of single-qubit Hilbert spaces $ mathcal{H}_2 = mathcal{H}_1 otimes mathcal{H}_1$ but we need to decide which qubit goes first and which goes second. In qiskit a convention is adopted that additional qubits join from the left, i.e. when we have two qubits as here . qc01 = QuantumCircuit(2) qc01.draw(output=&#39;mpl&#39;) . . The state of the system is $|q_1 rangle otimes |q_0 rangle$ (this is of course only true literally for non-entangled states but we can define everything only on the computational basis states ). OK, but how do we translate this into the matrix representation? The states in the tensor product of vector spaces can be represented by the Kronecker product which is not symmetric with respect to permutation arguments. Best way to explain how Kronecker product works is, as usual, through examples: . begin{align} begin{pmatrix} 1 0 end{pmatrix} otimes begin{pmatrix} a b end{pmatrix} = begin{pmatrix} a b 0 0 end{pmatrix}, qquad begin{pmatrix} 0 1 end{pmatrix} otimes begin{pmatrix} a b end{pmatrix} = begin{pmatrix} 0 0 a b end{pmatrix} end{align}Result for generic left vector can be obtained by linearity begin{align} begin{pmatrix} x y end{pmatrix} otimes begin{pmatrix} a b end{pmatrix} = x begin{pmatrix} 1 0 end{pmatrix} otimes begin{pmatrix} a b end{pmatrix} +y begin{pmatrix} 0 1 end{pmatrix} otimes begin{pmatrix} a b end{pmatrix} = begin{pmatrix} x a x b y a y b end{pmatrix} = begin{pmatrix} x begin{pmatrix} a b end{pmatrix} y begin{pmatrix} a b end{pmatrix} end{pmatrix} end{align} . The last notation here is a bit informal but it shows what happens. One just substitutes the right vector into all elements of the left vector, multiplied by the corresponding components of the left vector. The Kronecker product is defined in the same way for matrices of arbitrary size, not just for two vectors. . So, now we can compute matrix representations of states in the computation basis of two-qubit system . begin{align} |00 rangle = begin{pmatrix}1 0 end{pmatrix} otimes begin{pmatrix}1 0 end{pmatrix} = begin{pmatrix}1 0 0 0 end{pmatrix}, quad |01 rangle = begin{pmatrix}1 0 end{pmatrix} otimes begin{pmatrix}0 1 end{pmatrix} = begin{pmatrix}0 1 0 0 end{pmatrix} label{01} |10 rangle = begin{pmatrix}0 1 end{pmatrix} otimes begin{pmatrix}1 0 end{pmatrix} = begin{pmatrix}0 0 1 0 end{pmatrix}, quad |11 rangle = begin{pmatrix}0 1 end{pmatrix} otimes begin{pmatrix}0 1 end{pmatrix} = begin{pmatrix}0 0 0 1 end{pmatrix} end{align}There is a useful relation between the index of the non-zero element $n$ in the four-dimensional representation and the computational basis bitstring $q_1q_0$, namely $n=2q_1+q_0$. I.e. the bitstring $q_1q_0$ is the binary representation of the index $n$. This extends to arbitrary number of qubits, for example since $101$ is $5$ in binary representation it follows begin{align} |101 rangle = begin{pmatrix}0 0 0 0 0 1 0 0 end{pmatrix} label{101} end{align} (try to obtain this from the two tensor products!) . Don&#39;t believe me? OK, let&#39;s check! In qiskit there is a convenient function to construct a vector representation from a bit string which we will take advantage of. First start with a two-qubit example: . s01 = Statevector.from_label(&#39;01&#39;) s01.data . array([0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]) . Comparing to eqref{01} we find agreement. Similarly, . s101 = Statevector.from_label(&#39;101&#39;) s101.data . array([0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]) . Again, this is in agreement with eqref{101}. . However, I am not sure that this relation is sufficient to justify the ordering of the tensor products. To me it is much more natural to read the circuit from top to bottom and construct the Hilbert spaces accordingly, say $ mathcal{H}_0 otimes mathcal{H}_1 otimes mathcal{H}_2 dots$ instead of $ cdots mathcal{H}_2 otimes mathcal{H}_1 otimes mathcal{H}_0$. Later I will change the ordering of the tensor product to my liking, but for now we stick with the qiskit one. Now, with conventions for states in place we can proceed to operators. . Operators: horizontal ordering . One can say that convention for states representation and ordering of tensor products is a &quot;vertical&quot; convention. There is also a &quot;horizontal&quot; convention which might be potentially confusing. Consider the following circuit . qc123 = QuantumCircuit(1) qc123.rx(1, 0) qc123.ry(2, 0) qc123.rz(3, 0) qc123.draw(output=&#39;mpl&#39;) . . Here, the operator $R_x$ is appplied first, the operator $R_y$ second and $R_z$ last. So in mathematical notation the circuit corresponds to $R_z R_y R_x$ and not to $R_x R_y R_z$. I think that the circuit notation is actually better. We think and write from left to right, this is also a direction that time flows on paper. When another thing happens, we write it to the right and it would be convenient to apply the corresponding operator also to the right. I heard real mathematicians complain about that issue, but I guess we are stuck with it for now. . Paper-and-pencil computation . With the set up in place we can compute the circuit of interest by hands. For convenience I plot it here once again: . qc.draw(output=&#39;mpl&#39;) . . OK, so what is the unitary matrix corresponding to this circuit? It is begin{align} U = CNOT_{01} cdot (Y otimes X) end{align} Here begin{multline} CNOT_{01} = mathbb{1} otimes |0 rangle langle 0|+X otimes |1 rangle langle 1|= begin{pmatrix}1&amp;0 0&amp;1 end{pmatrix} otimes begin{pmatrix}1&amp;0 0&amp;0 end{pmatrix}+ begin{pmatrix}0&amp;1 1&amp;0 end{pmatrix} otimes begin{pmatrix}0&amp;0 0&amp;1 end{pmatrix}= begin{pmatrix}1&amp;0&amp;0&amp;0 0&amp;0&amp;0&amp;1 0&amp;0&amp;1&amp;0 0&amp;1&amp;0&amp;0 end{pmatrix} end{multline} and begin{align} Y otimes X = begin{pmatrix} 0&amp; -i i&amp;0 end{pmatrix} otimes begin{pmatrix} 0&amp; 1 1&amp;0 end{pmatrix}= begin{pmatrix}0&amp;0&amp;0&amp;-i 0&amp;0&amp;-i&amp;0 0&amp;i&amp;0&amp;0 i&amp;0&amp;0&amp;0 end{pmatrix} end{align} Multiplying them together gives begin{align} U = begin{pmatrix}0 &amp; 0 &amp; 0 &amp; -i i&amp;0&amp;0&amp;0 0 &amp; i &amp; 0 &amp; 0 0 &amp; 0 &amp; -i &amp; 0 end{pmatrix} end{align} Alright, so this is indeed the matrix that qiskit computes: . U_qs . array([[0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j], [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j], [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j], [0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j]]) . We can now also check that that the states evolve as we expected. For example recall that we computed that our quantum circuit maps $q_0 =|0 rangle, q_1 =|1 rangle$ to $q_0 =|1 rangle, q_1 =|1 rangle$ with an overall phase $-i$. Agreement with qiskit can be checked as follows: . qs_state = Statevector.from_label(&#39;10&#39;).evolve(qc).data our_state = -1j*Statevector.from_label(&#39;11&#39;).data np.allclose(qs_state, our_state) . True . Implementation with tensornetworks . I will not give a proper introduction to tensor networks but just make some digressions I think should be helpful as we go along. . First thing we will need are the matrices defining $X, Y$ and $CNOT$ gates. Let us introduce them. . X = np.array([[0, 1], [1, 0]]) Y = np.array([[0, -1j], [1j, 0]]) CNOT = np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]).reshape(2,2,2,2) . An aside about reshaping . Note that as usually written, $CNOT$ is a $4 times4$ matrix. Since as a quantum gate it acts on two qubits, so it should rather be a four-legged tensor. This is the purpose of the reshaping operation. At first the reshaping might be a bit tricky, so let me illustrate it with an example. Introduce two $4 times4$ matrices and define their product: . A = np.random.rand(4,4) B = np.random.rand(4,4) AB = A @ B . Now define the corresponding four-legged tensors. . import tensornetwork as tn a = tn.Node(A.reshape(2,2,2,2)) b = tn.Node(B.reshape(2,2,2,2)) . By contracting the legs (or &quot;edges&quot; in terminology of tensornetworks) appropriately, we can reproduce the matrix multiplication. First the code: . a[2] ^ b[0] a[3] ^ b[1] ab = tn.contractors.greedy([a, b], output_edge_order=[a[0], a[1], b[2], b[3]]).tensor . We can check that the contraction performed in this way exactly reproduces the matrix multiplication of original $4 times4$ matrices: . np.allclose(AB, ab.reshape(4,4)) . True . This can be interpreted graphically as follows. First, the reshaping procedure can be thought of as splitting each of two four-dimensional legs of the original matrix into two two-dimensional ones . . The labels on the legs have nothing to do with qubit states, these are just indices of edges as assigned by tn.Node operation on our matrices. The matrix multiplication of the original matrices in terms of four-legged tensors then can be drawn as follows . . The index arrangements in the last part explain why we connected the edges in our code the way we did. This is something to watch out for. For example, connecting edges of two identity tensors in the wrong way may produce a $SWAP$ gate. . Tensor product ordering . The matrix representation of a tensor diagram like this . . also comes with a convention for the ordering of tensor products. In tensornetwork as well as in my opinion it is natural to order top-down, i.e. the above diagram is $U otimes mathbb{1}$ instead of $ mathbb{1} otimes U$ as is adopted in qiskit. . Circuit from tensor network . Alright, not we are in a position to reproduce the circuit unitary from the tensor network with nodes x, y and cnot: . # Make tensors from matrices x, y, cnot = list(map(tn.Node, [X, Y, CNOT])) # Connect edges properly cnot[2] ^ y[0] cnot[3] ^ x[0] # Perform the contraction ~ matrix multiplication U_tn = tn.contractors.greedy([cnot, x, y], output_edge_order=[cnot[0], cnot[1], y[1], x[1]]).tensor . This way of contracting the edges corresponds to the following diagram: . . Note that this is basically the original circuit with both the vertical and the horizontal directions reversed. The horizontal reversal is due to mathematical vs circuit notation (circuit is better!) and the vertical reversal is due to the mismatch between qiskit and tensornetwork ordering of tensor product (tensornetwork&#39;s is better!). We can check that the unitary we obtain from this tensor network agrees with qiskit&#39;s . np.allclose(U_tn.reshape(4,4), U_qs) . True . A better way . I find all this misalignment very inconvenient and hard to debug. Ideally I want to look at the quantum circuit and construct the corresponding tensor network just as I read a text: from left to right and from top to bottom. Here I propose a solution which seems much more satisfactory to me. We will deal with horizontal reversal by first defining edges and then applying gates to them. This way we can read the circuit from left to right and simply add new gates, just as in qiskit. I will not try to revert the vertical direction directly, because I find it hard to think upside down. Instead, for comparison with qiskit I will use a built-in reverse_bits method. . So let&#39;s start by defining a function that applies a given gate to the collection of qubits (this is a slight modification of an example from tensornetwork docs) : . def apply_gate(qubits, gate_tensor, positions): gate = tn.Node(gate_tensor) assert len(gate.edges) == 2*len(positions), &#39;Gate size does not match positions provided.&#39; for i, p in enumerate(positions): # Connect RIGHT legs of the gate to the active qubits gate[i+len(positions)] ^ qubits[p] # Reassing active qubits to the corresponding LEFT legs of the gate qubits[p] = gate[i] . Importantly, here, in contrast to the official docs, we append the gate from the left, so that a sequence of application of some $G_1$ followed by $G_2$ is equivalent to the application of $G_2 cdot G_1$. Now there is one more subtlety. Previously we used matrix representation of $CNOT$ assuming that the uppermost qubit comes last in the tensor product. Now that we decided to turn this convention upside down our matrix representation of $CNOT$ must be $CNOT =|0 rangle langle 0| otimes mathbb{1}+|1 rangle langle 1| otimes X$ or explicitly . CNOT = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]).reshape(2,2,2,2) . With that we are ready to reconstruct our original circuit in a convenient way: . # The context manager `NodeCollection` is a bit of a magic trick # which keeps track of all tensors in the network automatically. all_nodes = [] with tn.NodeCollection(all_nodes): # I do not know how to create &#39;abstract&#39; edges in `tensornetworks`. # Instead, I create an identity tensor and use its edges to apply new gates to. id0 = tn.Node(np.identity(4).reshape(2,2,2,2)) qubits0 = id0.edges[2:4] qubits = id0.edges[0:2] apply_gate(qubits, X, [0]) apply_gate(qubits, Y, [1]) apply_gate(qubits, CNOT, [0,1]) . Now let us check! . U_tn = tn.contractors.greedy(all_nodes, output_edge_order=qubits+qubits0).tensor.reshape(4,4) U_reversed_qs = Operator(qc.reverse_bits()).data np.allclose(U_tn, U_reversed_qs) . True . Wohoo, it worked! If that looked simple to you I&#39;m happy. It took me several hours of debugging to finally match the two matrices. Just to make sure, let me conclude with a more complicated example. . qc3 = QuantumCircuit(3) qc3.x(0) qc3.cx(0, 1) qc3.y(1) qc3.x(2) qc3.cx(2, 1) qc3.y(2) qc3.draw(output=&#39;mpl&#39;) . As you can see, constructing the tensor network analog now works more or less identically: . all_nodes = [] with tn.NodeCollection(all_nodes): id0 = tn.Node(np.identity(8).reshape(2,2,2,2,2,2)) qubits0 = id0.edges[3:6] qubits = id0.edges[0:3] # The essential part apply_gate(qubits, X, [0]) apply_gate(qubits, CNOT, [0, 1]) apply_gate(qubits, Y, [1]) apply_gate(qubits, X, [2]) apply_gate(qubits, CNOT, [2, 1]) apply_gate(qubits, Y, [2]) . And now we compare: . U3_tn = tn.contractors.greedy(all_nodes, output_edge_order=qubits+qubits0).tensor.reshape(8,8) U3_qs_reversed = Operator(qc3.reverse_bits()).data np.allclose(U3_tn, U3_qs_reversed) . True . Alright, this resounding True is the best way to conclude that comes to mind. I own many thanks to Ilia Luchnikov for the help with tensornetwork library. Any questions are welcome in the comments! .",
            "url": "https://idnm.github.io/blog/blog/qiskit/tensor%20networks/quantum%20concepts/2021/08/18/Matrix-representation-of-quantum-circuits.html",
            "relUrl": "/qiskit/tensor%20networks/quantum%20concepts/2021/08/18/Matrix-representation-of-quantum-circuits.html",
            "date": " • Aug 18, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Machine learning compilation of quantum circuits",
            "content": "Introduction . I am going to review a recent preprint by Liam Madden and Andrea Simonetto that uses techniques from machine learning to tackle the problem of quantum circuits compilation. I find the approach suggested in the paper very interesting and the preliminary results quite promising. . What is compilation? . Note that a variety of terms are floating around the literature and used more or less interchangibly. Among those are synthesis, compilation, transpilation and decomposition of quantum circuits. I will not make a distinction and try to stick to compilation. . But first things first, what is a compilation of a quantum circuit? The best motivation and illustration for the problem is the following. Say you need to run a textbook quantum circuit on a real hardware. The real hardware usually allows only for a few basic one and two qubit gates. In contrast, your typical textbook quantum circuit may feature (1) complex many-qubit gates, for example multi-controlled gates and (2) one and two qubit gates which are not supported by the hardware. As a simple example take this 3-qubit Grover&#39;s circuit (from qiskit textbook): . #initialization import matplotlib.pyplot as plt import numpy as np # importing Qiskit from qiskit import IBMQ, Aer, assemble, transpile from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister from qiskit.providers.ibmq import least_busy # import basic plot tools from qiskit.visualization import plot_histogram def initialize_s(qc, qubits): &quot;&quot;&quot;Apply a H-gate to &#39;qubits&#39; in qc&quot;&quot;&quot; for q in qubits: qc.h(q) return qc def diffuser(nqubits): qc = QuantumCircuit(nqubits) # Apply transformation |s&gt; -&gt; |00..0&gt; (H-gates) for qubit in range(nqubits): qc.h(qubit) # Apply transformation |00..0&gt; -&gt; |11..1&gt; (X-gates) for qubit in range(nqubits): qc.x(qubit) # Do multi-controlled-Z gate qc.h(nqubits-1) qc.mct(list(range(nqubits-1)), nqubits-1) # multi-controlled-toffoli qc.h(nqubits-1) # Apply transformation |11..1&gt; -&gt; |00..0&gt; for qubit in range(nqubits): qc.x(qubit) # Apply transformation |00..0&gt; -&gt; |s&gt; for qubit in range(nqubits): qc.h(qubit) # We will return the diffuser as a gate U_s = qc.to_gate() U_s.name = &quot;U$_s$&quot; return U_s qc = QuantumCircuit(3) qc.cz(0, 2) qc.cz(1, 2) oracle_ex3 = qc.to_gate() oracle_ex3.name = &quot;U$_ omega$&quot; n = 3 grover_circuit = QuantumCircuit(n) grover_circuit = initialize_s(grover_circuit, [0,1,2]) grover_circuit.append(oracle_ex3, [0,1,2]) grover_circuit.append(diffuser(n), [0,1,2]) grover_circuit = grover_circuit.decompose() grover_circuit.draw(output=&#39;mpl&#39;) . . The three qubit gates like Toffoli are not generally available on a hardware and one and two qubit gates my be different from those in the textbook algorithm. For example ion quantum computers are good with Mølmer–Sørensen gates and may need several native one qubit gates to implement the Hadamard gate. . Additional important problem is to take into account qubit connectivity. Usually textbook algorithms assume full connectivity, meaning that two-qubit gates can act on any pair of qubits. On most hardware platforms however a qubit can only interact with its neighbors. Assuming that one and two qubits gates available on the hardware can implement a SWAP gate between adjacent qubits, to solve the connectivity problem one can insert as many SWAPs as necessary to connect topologically disjoint qubits. Using SWAPs however leads to a huge overhead in the number of total gates in the compiled circuit, and it is of much importance use them as economically as possible. In fact, the problem of optimal SWAPping alone in generic situation is NP-complete. . Simplified problem . When compiling a quantum circuit one has to decide which resulting circuits are considered to be efficient. Ideally, one should optimize for the total fidelity of the circuit. Let us imagine running the algorithm on a real device. Probably my theorist&#39;s image of a real device is still way too platonic, but I will try my best. Many details need to be taken into account. For example, gates acting on different qubits or pairs of qubits may have different fidelities. Decoherence of qubits with time can make circuits where many operations can be executed in parallel more favorable. Cross-talk (unwanted interactions) between neighboring qubits may lead to exotic patterns for optimal circuits. A simple proxy for the resulting fidelity that is often adopted is the number of two-qubit gates (which are generically much less accurate than a single-qubit gates). So the problem that is often studied, and that is addressed in the preprint we are going to discuss, is the problem of optimal compilation into a gate set consisting of arbitrary single-qubit gates and CNOTs, the only two qubits gate. The compiled circuit must . Respect hardware connectivity. | Have as few CNOTs as possible. | Exceed a given fidelity threshold. | Last item here means that we also allow for an approximate compilation. By increasing the number of CNOTs one can always achieve an exact compilation, but since in reality each additional CNOT comes with its own fidelity cost this might not be a good trade-off. Note also that a specific choice for two-qubit gate is made, a CNOT gate. Any two-qubit gate can be decomposed into at most 3 CNOTs see e.g. here, so in terms of computational complexity this is of course inconsequential. However in the following discussion we will care a lot about constant factors and may wish to revisit this choice at the end. . Existing results . Since finding the exact optimal solution to the compilation problem is intractable, as with many things in life one needs to resort to heuristic methods. A combination of many heuristic methods, in fact. As an example one can check out the transpilation workflow in qiskit. Among others, there is a step that compiles &gt;2 qubit gates into one and two qubit gates; the one that tries to find a good initial placement of the logical qubits onto physical hardware; the one that &#39;routes&#39; the desired circuit to match a given topology being as greedy on SWAPs as possible. Each of these steps can use several different heuristic optimization algorithms, which are continuously refined and extended (for example this recent preprint improves on the default rounting procedure in qiskit). In my opinion it would be waay better to have one unified heuristic for all steps of the process, especially taking into account that they are not completely independent. Although this might be too much to ask for, some advances are definitely possible and machine learning tools might prove very useful. The paper we are going to discuss is an excellent demonstration. . Theoretical lower bound and quantum Shannon decomposition . There is a couple of very nice theoretical results about the compilation problem that I need to mention. But first, let us agree that we will compile unitaries, not circuits. What is the difference? Of course, any quantum circuit (without measurements and neglecting losses) corresponds to a unitary matrix. However, to compute that unitary matrix for a large quantum circuit explicitly is generally an intractable problem, precisely for the same reasons that quantum computation is assumed to be more powerful than classical. Still, taking as the input a unitary matrix (which is in general hard to compute from the circuit) is very useful both theoretically and practically. I will discuss pros and cons of this approach later on. . OK, now the fun fact. Generically, one needs at least this many CNOTs . begin{align} L:= # text{CNOTs} geq frac14 left(4^n-3n-1 right) label{TLB} end{align}to exactly compile an $n$-qubit unitary. &#39;Generically&#39; means that the set of $n$-qubit unitaries that can be compiled exactly with smaller amount of CNOTs has measure zero. Keep in mind though, that there are important unitaries in this class like multi-controlled gates or qubit permutations. We will discuss compilation of some gates from the &#39;measure-zero&#39; later on. . The authors of the preprint (I hope you and me still remember that there is some actual results to discuss, not just my overly long introduction to read) refer to eqref{TLB} as the theoretical lower bound or TLB for short. The proof of this fact is actually rather simple and I will sketch it. A general $d times d$ unitary has $d^2$ real parameters. For $n$ qubits $d=2^n$. Single one-qubit gate has 3 real parameters. Any sequence of one-qubit gates applied to the same qubit can be reduced to a single one-qubit gate and hence can have no more than 3 parameters. That means, that without CNOTs we can only have 3n parameters in our circuit, 3 for each one-qubit gate. This is definitely not enough to describe an arbitrary unitary on $n$ qubits which has $d^2=4^n$ parameters. . Now, adding a single CNOT allows to insert two more 1-qubit unitaries after it, like that . from qiskit.circuit import Parameter a1, a2, a3 = [Parameter(a) for a in [&#39;a1&#39;, &#39;a2&#39;, &#39;a3&#39;]] b1, b2, b3 = [Parameter(b) for b in [&#39;b1&#39;, &#39;b2&#39;, &#39;b3&#39;]] qc = QuantumCircuit(2) qc.cx(0, 1) qc.u(a1, a2, a3, 0) qc.u(b1, b2, b3, 1) qc.draw(output=&#39;mpl&#39;) . . At the first glance this allows to add 6 more parameters. However, each single-qubit unitary can be represented via the Euler angles as a product of only $R_z$ and $R_x$ rotations either as $U=R_z R_x R_z$ or $U=R_x R_y R_z$ (I do not specify angles). Now, CNOT can be represented as $CNOT=|0 rangle langle 0| otimes I+|1 rangle langle 1| otimes X$. It follows that $R_z$ commutes with the control of CNOT and $R_x$ commutes with the target of CNOT, hence they can be dragged to the left and joined with preceding one-qubit gates. So in fact each new CNOT gate allows to add only 4 real parameters: . a1, a2 = [Parameter(a) for a in [&#39;a1&#39;, &#39;a2&#39;]] b1, b2 = [Parameter(b) for b in [&#39;b1&#39;, &#39;b2&#39;]] qc = QuantumCircuit(2) qc.cx(0, 1) qc.rx(a1, 0) qc.rz(a2, 0) qc.rz(b1, 1) qc.rx(b2, 1) qc.draw(output=&#39;mpl&#39;) . . That&#39;s it, there are no more caveats. Thus, the total number of parameters we can get with $L$ CNOTs is $3n+4L$ and we need to describe a $d times d$ unitary which has $4^n$ parameters. In fact, the global phase of the unitary is irrelevant so we only need $3n+4L geq 4^n-1$. Solving for $L$ gives the TLB eqref{TLB}. That&#39;s pretty cool, isn&#39;t it? . Now there is an algorithm, called quantum Shannon decomposition (see ref), which gives an exact compilation of any unitary with the number of CNOTs twice as much as the TLB requires. In complexity-theoretic terms an overall factor of two is of course inessential, but for current NISQ devices we want to get as efficient as possible. Moreover, to my understanding the quantum Shannon decomposition is not easily extendable to restricted topology while inefficient generalizations lead to a much bigger overhead (roughly an order of magnitude). . What&#39;s in the preprint? . Templates . I&#39;ve already wrote an introduction way longer than intended so from now on I will try to be brief and to the point. The authors of the preprint propose two templates inspired by the quantum Shannon decomposition. The building block for each template is a &#39;CNOT unit&#39; . a1, a2 = [Parameter(a) for a in [&#39;a1&#39;, &#39;a2&#39;]] b1, b2 = [Parameter(b) for b in [&#39;b1&#39;, &#39;b2&#39;]] qc = QuantumCircuit(2) qc.cx(0, 1) qc.ry(a1, 0) qc.rz(a2, 0) qc.ry(b1, 1) qc.rx(b2, 1) qc.draw(output=&#39;mpl&#39;) . . First template is called sequ in the paper and is obtained as follows. There are $n(n-1)/2$ different CNOTs on $n$-qubit gates. We enumerate them somehow and simply stack sequentially. Here is a 3-qubut example with two layers (I use qiskit gates cz instead of our &#39;CNOT units&#39; for the ease of graphical representation) . qc = QuantumCircuit(3) for _ in range(2): qc.cz(0, 1) qc.cz(0, 2) qc.cz(1, 2) qc.barrier() qc.draw(output=&#39;mpl&#39;) . . The second template is called spin and for 4 qubits looks as follows . qc = QuantumCircuit(4) for _ in range(2): qc.cz(0, 1) qc.cz(1, 2) qc.cz(2, 3) qc.barrier() qc.draw(output=&#39;mpl&#39;) . . I&#39;m sure you get the idea. That&#39;s it! The templates fix the pattern of CNOTs while angles of single-qubit gates are adjustable parameters which are collectively denoted by $ theta$. . The idea now is simple. Try to optimize these parameters to achieve the highest possible fidelity for a given target unitary to compile. I am not at all an expert on the optimization methods, so I might miss many subtleties, but on the surface the problem looks rather straightforward. You can choose your favorite flavor of the gradient descent and hope for convergence. The problem appears to be non-convex but the gradient descent seems to work well in practice. One technical point that I do not fully understand is that the authors choose to work with fidelity defined by the Frobenius norm $||U-V||_F^2$ which is sensitive to the global phase of each unitary. To my understanding they often find that local minima of this fidelity coincides with the global minimum up to a global phase. OK, so in the rest of the post I refer to the &#39;gradient descent&#39; as the magic numerical method which does good job of finding physically sound minimums. . Results . Compiling random unitaries . OK, finally, for the surprising results. The authors find experimentally that both sequ and spin perform surprisingly well on random unitaries always coming very close to the TLB eqref{TLB} with good fidelity. More precisely, the tests proceed as follows. First, one generates a random unitary. Next, for each number $L$ of CNOTs below the TLB one runs the gradient descent to see how much fidelity can be achieved with this amount of CNOTs. Finally, one plots the fidelity as a function of $L$. Impressively, on the sample of hundred unitaries the fidelity always approaches 100% when the number of CNOTs reaches the TLB. For the $n=3$ qubits TLB is $L=14$, for $n=5$ $L=252$ (these are the two cases studied). So, in all cases studied, the gradient descent lead by the provided templates seems to always find the optimal compilation circuit! Recall that this is two times better than quantum Shannon decomposition. Please see the original paper for nice plots that I do not reproduce here. . Compiling on restricted topology . These tests were performed on the fully connected circuits. The next remarkable discovery is that restricting the connectivity does not to seem to harm the performance of the compilation! More precisely, the authors considered two restricted topologies in the paper, &#39;star&#39; where all qubits are connected to single central one and &#39;line&#39; where well, they are connected by links on a line. The spin template can not be applied to star topology, but it can be applied to line topology. The sequ template can be generalized to any topology by simply omitting CNOTs that are not allowed. Again, as examining a hundred of random unitaries on $n=3$ and $n=5$ qubits shows, the fidelity nearing 100% can be achieved right at the TLB in all cases, which hints that topology restriction may not be a problem in this approach at all! To appreciate the achievement, imagine decomposing each unitary via the quantum Shannon decomposition and then routing on restricted topology with swarms of SWAPs, a terrifying picture indeed. It would be interesting to compare the results against the performance of qiskit transpiler which is unfortunately not done in the paper to my understanding. . Compiling specific &#39;measure zero&#39; gates . Some important multi-qubit gates fall into the &#39;measure zero&#39; set which can be compiled with a smaller amount of CNOTs than is implied by the TLB eqref{TLB}. For example, 4-qubit Toffoli gate can be compiled with 14 CNOTs while the TLB requires 61 gates. Numerical tests show that the plain version of the algorithm presented above does not generically obtain the optimal compilation for special gates. However, with some tweaking and increasing the amount of attempts the authors were able to find optimal decompositions for a number of known gates such as 3- and 4-qubit Toffoli, 3-qubit Fredkin and 1-bit full adder on 4 qubits. The tweaking included randomly changing the orientation of some CNOTs (note that in both sequ and spin the control qubit is always at the top) and running many optimization cycles with random initial conditions. The best performing method appeared to be sequ with random flips of CNOTs. The whole strategy might look a bit fishy, but I would argue that it is not. My argument is simple: you only need to find a good compilation of the 4-qubit Toffoli once. After that you pat yourself on the back and use the result in all your algorithms. So it does not really matter how hard it was to find the compilation as long as you did not forget to write it down. . Compressing the quantum Shannon decomposition . Finally, as a new twist on the plot the authors propose a method to compress the standard quantum Shannon decomposition (which is twice the TLB, remember?). The idea seems simple and works surprisingly well. The algorithm works as follows. . Compile a unitary exactly using the quantum Shannon decomposition. | Promote parameters in single-qubit gates variables (they have fixed values in quantum Shannon decomposition). | Add LASSO-type regularization term, which forces one-qubit gates to have small parameters, ideally zero (which makes the corresponding gates into identities). | Run a gradient descent on the regularized cost function (fidelity+LASSO term). Some one-qubit gates will become identity after that (one might need to tune the regularization parameter here). | After eliminating identity one-qubit gates one can end up in the situation where there is a bunch of CNOTs with no single-qubit gates in between. There are efficient algorithms for reducing the amount of CNOTs in this case. | Recall that the fidelity was compromised by adding regularization terms. Run the gradient descent once more, this time without regularization, to squeeze out these last percents of fidelity. | From the description of this algorithm it does not appear obvious that the required cancellations (elimination of single-qubit gates and cancellations in resulting CNOT clusters) is bound to happen, but the experimental tests show that they do. Again, from a bunch of random unitaries it seems that the $ times 2$ reduction to the TLB is almost sure to happen! Please see the preprint for plots. . Weak spots . Although I find results of the paper largely impressive, a couple of weak spots deserve a mention. . Limited scope of experiments . The numerical experiments were only carried out for $n=3$ and $n=5$ qubits which of course is not much. To see if the method keeps working as the number of qubits is scaled is sure very important. There may be two promblems. First, the templates can fail to be expressive enough for larger circuits. The authors hope to attack this problem from the theoretical side and show that the templates do fill the space of unitaries. Well, best of luck with that! Another potential problem is that although the templates work fine for higher $n$, the learning part might become way more challenging. Well, I guess we should wait and see. . Unitary as the input . As I discussed somewhere way above, for a realistic quantum computation we can not know the unitary matrix that we need to compile. If we did, there would no need in the quantum computer in the first place. I can make two objects here. First, we are still in the NISQ era and pushing the existing quantum computers to their edge is a very important task. Even if an algorithm can be simulated classically, running it on a real device might be invaluable. Second, even quantum circuits on 1000 qubits do not usually feature 100-qubit unitaries. So it could be possible to separate a realistic quantum circuit into pieces, each containing only a few qubits, and compile them separately. . Final remarks . To me, the algorithms presented in the preprint seem to be refreshingly efficient and universal. At some level it appears to be irrelevant which exact template do we use. Near the theoretical lower bound they all perform similarly well, even on restricted topology. This might be a justification for choosing CNOT as the two-qubit gate, as this probably does not matter in the end! I&#39;m really cheering for a universal algorithm like that to win the compilation challenge over a complicated web of isolated heuristics, which are currently state of the art. .",
            "url": "https://idnm.github.io/blog/blog/machine%20learning/compilation/qiskit/paper%20review/2021/07/22/Machine-learning-compilation-of-quantum-circuits.html",
            "relUrl": "/machine%20learning/compilation/qiskit/paper%20review/2021/07/22/Machine-learning-compilation-of-quantum-circuits.html",
            "date": " • Jul 22, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "What is entanglement?",
            "content": "Introduction . I&#39;ve known the formal definition of entanglement for years, but I am only now appreciating many of its profound implications. In this post I would like to share two aspects that put entangled states into sharp contrast with unentangled (separable pure) states and classical random variables. Instead of proofs I provide references and simple experiments in qiskit. . . Entanglement is the failure of states to factorize . So what is entanglement? Entanglement is what entangled states have. What are those? Take two spins. The state . begin{equation} frac1{ sqrt{2}} Big(| uparrow uparrow rangle+| downarrow downarrow rangle Big) label{bell} end{equation}is your canonical example of an entangled stated. In contrast, all the states below are unentangled begin{align} | uparrow uparrow rangle, qquad | downarrow uparrow rangle, qquad frac1{ sqrt{2}}| uparrow rangle Big(| uparrow rangle-| downarrow rangle Big), qquad frac1{ sqrt{2}} Big(| uparrow rangle-| downarrow rangle Big) Big(| uparrow rangle+| downarrow rangle Big) label{unen} end{align} . The difference between eqref{bell} and eqref{unen} is the following. All latter states are actually products of the form $| psi_1 rangle | psi_2 rangle$ where $| psi_1 rangle$ is the state of the first system and $| psi_2 rangle$ of the second. In contrast, state eqref{bell} can not be represented in as a product. It is instead a linear combination of factorized states which is not reducible to a single product. You can define entangled states by this property of not being factorizible into states of consistuent spins. . Now that we know what entangled states are it is perfectly reasonable to ask: &quot;so what?&quot;. Why are entangled states special? I am going to give two angles on this questions, out of many possible. . . Note for the sake of concreteness and simplicity I talk about &quot;spins&quot;. In the context of discrete-variable quantum computation &quot;spin&quot;$ equiv$&quot;qubit&quot;, but I prefer spins, because they come with a useful geometrical intuition. The abstract Bloch sphere associated to a qubit describes an actual orientation of a spin in $3d$ space. . Entangled spin behaves very differently from unentangled . A spin which is not entangled can always be described by a direction $ bf n$ along which it is pointing $| uparrow_{ bf n} rangle$. If one measures the component of the spin along this direction, the result is always $ frac12$. Such a measurement corresponds to a projector $P({ bf n})={ bf n} cdot { bf sigma}=n_x sigma_x+n_y sigma_y+n_z sigma_z$. If state $| uparrow_{ bf n} rangle$ is measured along a different axis $ bf n&#39;$ the result depends on the angle $ theta$ between $ bf n$ and $ bf n&#39;$. With probability $ cos^2 frac theta2$ one gets projection $+ frac12$ and with probability $ sin^2 frac theta2$ one gets $- frac12$. However, for any state of the spin $| psi rangle$ there is an axis $ bf n$, such that measuring the spin along this axis gives $ frac12$ with probability one. . This is also true for any of the unentangled states eqref{unen}. For example, measuring the projection of the first spin in the state $| uparrow uparrow rangle equiv | uparrow_{ bf z} uparrow_{ bf z} rangle$ along $ bf z$ always gives $+ frac12$. As another example, since begin{align} | downarrow_{ bf x} rangle= frac12 Big(| uparrow_{ bf z} rangle-| downarrow_{ bf z} rangle Big) label{xdown} end{align} the state $ frac1{ sqrt{2}} Big(| uparrow rangle-| downarrow rangle Big) Big( uparrow rangle+| downarrow rangle Big)$ always registers $- frac12$ when the projection of the first spin along $ bf x$ axis is measured. . In contrast, for the maximally entangled state eqref{bell} the axis with a definite projection of the first spin does not exist. In fact, for all intents and purposes, if you only look at observables associated with the first qubit, state eqref{bell} behaves as a statistical ensemble of states $| uparrow rangle$ and $| downarrow rangle$, i.e. . begin{align} frac1{ sqrt{2}} Big(| uparrow uparrow rangle+| downarrow downarrow rangle Big) approx cases{| uparrow rangle text{ with probability $ frac12$} | downarrow rangle text{ with probability $ frac12$}} label{bellapprox} end{align}This means, for example, that projection onto $ bf z$ axis of the first spin is completely random: with probability $ frac12$ it behaves as $| uparrow rangle$ and gives projection $+ frac12$, with probability $ frac12$ it behaves as $| downarrow rangle$ and gives projection $- frac12$. This is different from a coherent superposition of the up and down states, such as eqref{xdown}. Although state eqref{xdown} gives random results when measured along $ bf z$, it gives certain results when measured along $ bf x$. There is no such axis for state eqref{bellapprox}. In fact, the spin projection along any axis is completely random. . To prove this fact I would need to go into some details of how one does construct an ensemble from an entangled state. This is not at all difficult but I won&#39;t do it here. I encourage an interested reader to consult John Preskill&#39;s notes (chapter 2.3). . Instead, let me do a quick experimental check using qiskit. A Hadamard gate followed by a CNOT creates our state eqref{bell}: . from qiskit import QuantumCircuit, BasicAer, execute from qiskit.visualization import plot_histogram qc = QuantumCircuit(2, 1) qc.h(0) qc.cx(0, 1) qc.draw(output=&#39;mpl&#39;) . To my knowledge, one can only measure in the computational basis in qiskit, i.e. only along $ bf z$ axis in our terminology. To measure a spin along some axis $ bf n$ we can instead rotate the spin itself, and then measure along $ bf z$ axis. Mathematically, if ${ bf n} = R^{-1} { bf z}$ for some rotation $R$ then $ langle uparrow_{ bf z}|P({ bf n})| uparrow_ rangle= langle uparrow_{R{ bf z}}|P({ bf z})| uparrow_{R{ bf z}} rangle$. . # Feel free to change them and see if the outcome distribution changes. theta, pi, lam = 0.13, 0.89, 0.37 qc.u(theta, pi, lam, 0) # Rotate the qubit. qc.measure(0, 0) # Execute on a simulator and plot a histogram of the result. backend = BasicAer.get_backend(&#39;qasm_simulator&#39;) result = execute(qc, backend, shots=1000).result() counts = result.get_counts(qc) plot_histogram(counts) . The result looks like a fair sample from the uniform probability distribution. This means that projection on the axis we have specified is indeed random. You can try to change the axis and see if you can get a biased distribution (spoiler: you can not). . Entanglement correlations are stronger than classical . First let me note that although we talked about the first spin before, the state eqref{bell} is symmetric and everything equally applies to the second spin. Although the behavior of each of these spins is completely random, there are strong correlations between the them. If we can make local measurements on both spins the state eqref{bell} behaves as . begin{align} frac1{ sqrt{2}} Big(| uparrow uparrow rangle+| downarrow downarrow rangle Big) approx cases{| uparrow uparrow rangle text{ with probability $ frac12$} | downarrow downarrow rangle text{ with probability $ frac12$}} label{bellapprox2} end{align}So for example projections onto $ bf z$ axis of both spins are always the same, although random. Again, this in fact holds for any axis. Here is an experimental verification. . qc = QuantumCircuit(2, 2) qc.h(0) qc.cx(0, 1) # Rotation of each qubit to simulate measurement along arbitary axis. theta, pi, lam = 0.13, 0.89, 0.37 qc.u(theta, pi, lam, 0) qc.u(theta, pi, lam, 1) qc.measure([0, 1], [0, 1]) # Simulate and plot results. backend = BasicAer.get_backend(&#39;qasm_simulator&#39;) result = execute(qc, backend, shots=2000).result() counts = result.get_counts(qc) plot_histogram(counts) . The result I get is almost certainly a uniform distribution of over $00=| uparrow_{ bf n} uparrow_{ bf n} rangle$ and $11=| downarrow_{ bf n} downarrow_{ bf n} rangle$ (you can change $ bf n$ by changing angles in the code), however I also get a tiny number of spurious counts for $01$ and $10$, which is probably a bug, hm. . When seeing this for the first time there is definitely something to contemplate, like say an EPR paradox. Spoiler: it is not possible to use these correlations for superluminal transmission of information, but they are still a valuable resource. I will discuss just one manifestation of these quantum correlations which has a very concrete operational interpretation -- it allows a quantum team to play a certain probabilistic game better than any classical team could! Note that this is also basically Bell&#39;s theorem in disguise. . So here is the setup. Alice and Bob are playing together against Charlie. Charlie sends random uncorrelated bits $x$ to Alice and $y$ to Bob. Admittedly, Charlie&#39;s job is not very creative and nothing in his strategy can be changed. Now, in response to the obtained bits Alice produces her output bit $a$ and Bob his $b$. Team A&amp;B wins if $a oplus b=x land y$ where $ oplus$ is XOR (sum modulo 2) and $ land$ is the logical AND. Explicitly, if $x land y=1$ both Alice and Bob got $x=y=1$ (which happens one quarter of the time) and they win iff they respond $a=0, b=1$ or $a=1, b=0$ so that $a oplus b=1$. For all other inputs from Charlie, i.e. when $(x,y)$ is equal to $(0,0), (1,0)$ or $(0,1)$ the logical sum $x land y=0$ and Alice and Bob win iff $a=0,b=0$ or $a=1, b=1$ so that $a oplus b=0$. . Now, although in the same team, Alice and Bob are not allowed to communicate during the game. But they can discuss their strategy in advance. The best that a classical team can do is to win $75 %$ of the time. To achieve this winning rate it is sufficient to simply output $a=0, b=0$ irrespective of Charlie&#39;s bits $x,y$. This strategy only loses when $x=y=1$, i.e. one quarter of the time. . Now comes the interesting part. If Alice and Bob each have a spin, and these spins are entangled as in state eqref{bell}, they can achieve the winning probability begin{align} P_{win}= frac12+ frac1{2 sqrt{2}} approx 0.85! label{pwin} end{align} So, what should they do? . Define four axes $ bf n_1,n_2,n_3,n_4$ in the $ bf xz$ plane (of course this is just one of the possibilities). Take ${ bf n_1}= (1,0)$, then ${ bf n_2}=( frac1{ sqrt{2}}, frac1{ sqrt{2}})$ is counter-clockwise rotated by $ pi/4$ wrt to $ bf n_1$; ${ bf n_3}=(0,1)$ is rotated by $ pi/2$; and finally ${ bf n_4}=(- frac1{ sqrt{2}}, frac1{ sqrt{2}})$ is rotated by $3 pi/4$. . . Now here is the strategy that Alice and Bob follow begin{align} a(x)= cases{P_{ bf n_3}, qquad x=0 P_{ bf n_1}, qquad x=1} qquad qquad b(y)= cases{P_{ bf n_2}, qquad y=0 P_{ bf n_4}, qquad y=1} label{abcases} end{align} . Where $P_{ bf n}=+1$ if Alice&#39;s (or Bob&#39;s) spin gave projection $+ frac12$ when measured along $ bf n$ and $P_{ bf n}=0$ if the projection was $- frac12$. An example: if Alice recieves $x=0$ and Bob $y=1$ Alice measures her spin along $n_3= bf z$ axis and sends back the result, while Bob measures his spin along $ bf{n_4}$ (which is $3 pi/4$ rotated $ bf x$ axis) and sends his result. . Now, shall we check that this strategy indeed achieves the advertised winning probability eqref{pwin}? Sure, I also thought so! . import numpy as np # Define rotation axes by their angles. theta1 = 0 theta2 = np.pi/4 theta3 = np.pi/2 theta4 = 3*np.pi/4 def charlie(): # Charlies job is to generate two random bits. return np.random.randint(0,1+1, size=(2)) def alice(x): # Alice decides on the measurement axis according to her strategy. if x==0: return theta3 if x==1: return theta1 def bob(x): # Bob does his part of the protocol. if x==0: return theta2 if x==1: return theta4 def one_round(): # First we prepare an entangled state. qc = QuantumCircuit(2, 2) qc.h(0) qc.cx(0, 1) # Now Charlie generates his bits. x, y = charlie() # A&amp;B team makes their move. a_angle = alice(x) b_angle = bob(y) # Again, we can not measure directly along the desired axes, # but must rotate the qubits instead. Rotation in the xz plane is made by `ry` gate. qc.ry(a_angle, 0) # Alice rotates her qubit. qc.ry(b_angle, 1) # Bob his. # Now we add measurments and actually run the circuit. qc.measure([0, 1], [0, 1]) backend = BasicAer.get_backend(&#39;qasm_simulator&#39;) result = execute(qc, backend, shots=1).result() counts = result.get_counts(qc) # Output of counts is a dict like `{&#39;01&#39;: 1}`. This extracts the measurment results: a, b = [int(c) for c in list(counts.keys())[0]] # And now we check, team A&amp;B gogogo! return (a + b) % 2 == x * y . Alright, now let us collect the statistics: num_rounds = 2000 wins = 0 for _ in range(num_rounds): wins += one_round() print (&quot;Win probability:{}&quot;.format(wins/num_rounds)) . Win probability:0.847 . So that&#39;s pretty close to the theoretical value eqref{pwin}. Note that for each round of the game a new entangled pair is needed. . Now that we have seen that the strategy works let us briefly discuss why. I will only give a sketch and refer for details to Preskill&#39;s lectures chap 4.3. . One thing Alice and Bob could do is to always measure along the same axes. Then, their results would be perfectly correlated (i.e. they always output $a=b=0$ or $a=b=1$) which gives 0.75 winning probability, the same as the best deterministic strategy. Now, in one quarter of cases (when $x=y=1$) they are better off outputting anticorrelated results. If we revisit the figure above equation eqref{abcases} we see that the angle between $a(1)$ and $b(1)$ is $3 pi/4$ which indeed gives a negative correlation in this case $ Big( cos frac{3 pi}{4}=- frac{1}{ sqrt{2}} Big)$. The price to pay is that angles between $ Big(a(0),b(0) Big)$, $ Big(a(0),b(1) Big)$ and $ Big(a(1),b(0) Big)$ are now non-zero (and hence correlations are less than 1) which makes this strategy lose in some cases when the deterministic strategy wins. However, as we have seen experimentally the trade-off is still in our favor. It is also possible to prove that our choice of axes gives the maximum possible win probability. This is ultimately bound by Tsirelson&#39;s bound, see below. . Now you might ask -- what if there exists a clever randomized classical strategy which would perform better than deterministic 0.75 using a similar trick? Turns out this is not possible. The proof is based on the following inequality begin{align} Big| langle a_0 b_0 rangle+ langle a_0 b_1 rangle+ langle a_1 b_0 rangle- langle a_1 b_1 rangle Big| leq 2 end{align} which holds for any random variables $a_0, a_1, b_0, b_1$ taking values $ pm1$ and described by a joint probability distribution. This is known as CHSH inequality and a technical proof is trivial. Why quantum correlations do not have to obey the bound? Well, the reason is somewhat deep and quantum and ultimately related to Bohr&#39;s complementarity) -- non-commuting observables can not be simultaneously assigned values. That this statement has quantitative consequences is illustrated by Bell&#39;s theorem or our game. . Tehcnically quantum correlations obey the Tsirelson&#39;s bound begin{align} Big| langle a_0 b_0 rangle+ langle a_0 b_1 rangle+ langle a_1 b_0 rangle- langle a_1 b_1 rangle Big| leq 2 sqrt{2} end{align} which, as you see, is weaker by a factor $ sqrt{2}$, so the correlations themselves can be stronger, although still bounded. . Final remarks . Quantum entanglement is indeed very unusual and consequential. There are many more wonders that it entails, please consult your favorite lecture notes for a non-exhaustive list. My current favorite are John Preskill&#39;s lecture notes. For a non-mathematical although technically very accurate discussion of entanglement see this artice by Frank Wilczek entanglement made simple. . Any questions and suggestions are welcome, as this is my first blog demo. .",
            "url": "https://idnm.github.io/blog/blog/quantum%20concepts/qiskit/2021/07/12/Entanglement.html",
            "relUrl": "/quantum%20concepts/qiskit/2021/07/12/Entanglement.html",
            "date": " • Jul 12, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "How was this blog set up?",
            "content": "Why fastpages? . After deciding to start a scientific blog I was looking for an appropriate technical solution. My main requirements were . Ease of set up. | Ease of writing posts. | Decent support of $ LaTeX$. | Support of code snippets. | . After some search I decided to try out fastpages. I have a very limited understanding of the stack that fastpages use, so I treat it as a magic box. The magic box was easy for me to install while other bullet points are addressed all at once since fastpages allows to generate a post from a jupyter notebook. Although jupyter notebook is not exactly my favorite $ LaTeX$ editor it still much better than many other options and a good overall compromise. So essentially with fastpages you can write your posts in jupyter notebook, then commit to your github repository and the content will automatically be hosted at your domain on github pages. . Caveats . Following official installation worked smoothly for me. While customizing the blog further for my purposes there were several things that did not work right of the box of took some time to find out how to change: . Solved . I wanted to use numbered $ LaTeX$ equations with hyperlinks, which are not easily supported. This comment solved my problem! | You need to edit _pages/about.md to customize the way your &quot;about&quot; page is displayed. | To customize the front page you need to edit index.html. This is literally written on the front page of your blog, but I have not noticed it for a while. | Initially a lot of troubleshooting is needed to get the appearance of the blog I wanted. Commiting and waiting for the online web page to set up is super-slow. Here is an official guide on how to setup a live preview of your blog locally. One minor point that was a problem for me is that the default local server for blog preview https://127.0.0.1:4000 was not correct. After running sudo make server one of the outputs that jekyll produces is Server address: http://0.0.0.0:4000/blog/ which was the correct address for the live preview of my blog. | You need to do some work to make your site appear in google search results. This manual is very helpful, but a bit outdated: some of the things like generating sitemap.xml are now automated and do not require additional work as described in that post. | Not solved . On the web page the display equations of $ LaTeX$ have fluctuations in size which does not look good. |",
            "url": "https://idnm.github.io/blog/blog/fastpages/2021/07/11/How-this-blog-was-set-up.html",
            "relUrl": "/fastpages/2021/07/11/How-this-blog-was-set-up.html",
            "date": " • Jul 11, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "My name is Nikita Nemkov, I am a theoretical physicist diving into the field of quantum computation. On this blog I consolidate some of my thoughts on the subject, from reviews of the basic concepts to brief reports on the newest research papers. . I do not expect to have many readers and I do not tailor my posts to any particular audience. Perhaps the reader that could benefit from these notes the most is me, before writing them. Yet, if you found anything that I wrote useful give me a quick feedback in the comments or drop an email! .",
          "url": "https://idnm.github.io/blog/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://idnm.github.io/blog/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}